### 实验方法与原则

#### **训练集与测试集**
- **训练集（Training Set）**：
  - 模型可见样本标签，用于训练模型。
  - 样本数量有限，以确保模型能够学习到数据的模式和特征。
  - 潜在问题：可能存在过拟合，即模型在训练集上的表现好，但在其他未见样本上的表现差。
- **测试集（Test Set）**：
  - 用于评估模型在可能遇到的未见样本上的表现。
  - 尽可能与训练集互斥，即测试样本尽量不在训练集中出现，以确保测试集的独立性和鲁棒性。
  - 目标：估计模型在整个未见样本上的表现。

#### **训练集、验证集与测试集**
- 完整的数据集被分为三个部分：
  1. **训练集（Training Set）**：用于训练模型。
  2. **验证集（Validation Set）**：用于调整模型、优化超参数。
  3. **测试集（Test Set）**：用于评估最终模型的效果。
- 流程：
  1. 训练模型；
  2. 使用验证集调整模型和超参数；
  3. 再次训练模型；
  4. 使用测试集评估最终模型。

### 评价指标

在不同任务下衡量模型的性能有不同的评价指标。

#### **回归任务**
- **平均绝对误差（MAE）**：实际值与预测值之差的绝对值的平均值。
- **均方误差（MSE）**：实际值与预测值之差的平方的平均值。
- **均方根误差（RMSE）**：MSE的平方根。

#### **分类任务**
- **准确率（Accuracy）**：预测正确的样本数占总样本数的比例。
- **精度（Precision）**：在预测为正的样本中，实际为正的比例。
- **召回率（Recall）**：在实际为正的样本中，被预测为正的比例。

#### **特定任务**
- **个性化推荐系统**：
  - **前K项精度（Precision@K）**：前K项推荐中，正例（用户喜欢的项目）的比例。
  - **前K项召回率（Recall@K）**：前K项推荐中，正例数占候选集中所有正例的比例。
  - **前K项命中率（Hit@K）**：前K项推荐中是否有正例。
  - 其他指标：nDCG@K、点击率、用户留存、利润转化等。
- **对话系统**：
  - **BLEU、ROUGE、METEOR**：基于词、n-gram匹配衡量预测句子与目标句子之间的相似度。
  - 基于词向量计算预测句子与目标句子之间的相似度。
  - 其他指标：用户与系统对话的时长、次数、人工评价等。

#### **相关性指标**
- **Discounted Cumulative Gain (DCG)**：
  - 使用分级的相关性来衡量文档有用性或增益。
  - 增益从排序列表的开头开始累积，随着位次增加，增益可能会减弱（折损）。
  - 典型的折损函数为 \( \frac{1}{\log(rank)} \)。
  - 例如，底数为2时，位次4的折损为 \(\frac{1}{2}\)，位次8为 \(\frac{1}{3}\)。

#### **BLEU（Bilingual Evaluation Understudy）**
- 最初多用于机器翻译，后来也被其他任务借鉴（如对话生成等）。
- 检测译文中的每个n-gram是否在参考译文中出现。
- 精确率考虑词出现的次数限制，确保某个词在译文中的有效频次不应超过参考译文中的频次。

### 统计有效性检验

#### **比较算法的优劣**
- **显著性检验**：比较两个算法的平均准确率是否有显著差异。
- **样本量较大（\( n > 30 \)）**时，使用 **z检验**（基于中心极限定理）。
- **样本量较小（\( n <= 30 \)）**时，使用 **t检验**。

这些方法能帮助你评价模型在不同类型任务中的表现，并通过统计检验确认模型性能的显著性。