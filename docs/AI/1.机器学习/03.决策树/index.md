# 决策树学习基础

## 1. 决策树概述

决策树是一种常用的机器学习方法，特别适用于带有非数值特征的分类问题。它通过一系列规则对数据进行分割，形成树状结构，最终做出决策。

### 适用场景

- 带有非数值特征的分类问题
- 离散特征（如颜色、大小等类别型数据）
- 特征间没有明确的相似度概念
- 特征无序的情况

**示例**：判断水果类型

- 颜色：红色、绿色、黄色...
- 大小：小、中、大
- 形状：球形、细长
- 味道：甜、酸

## 2. 混杂度(不纯度)度量

决策树构建的核心是选择最佳分割特征，这需要衡量节点的不纯度(混杂度)。主要有三种度量方式：

### 2.1 熵(Entropy)

公式：
$$Entropy(N)=-\sum_{j} P\left(w_{j}\right) \log _{2} P\left(w_{j}\right)$$

- 表示系统的混乱程度
- 当所有类别概率相等时，熵最大
- 完全纯净时(只有一类)，熵为0

### 2.2 Gini指数

公式：
$$i(N)=\sum_{i ≠j} P(w_{i}) P(w_{j})=1-\sum_{j} P^{2}(w_{j})$$

- 表示随机抽取两个样本，它们类别不一致的概率
- 最大Gini混杂度在$1-n*(1/n)^{2}=1-1/n$时取得

### 2.3 错分类率

公式：
$$
i(N)=1-\max _{j} P(w_{j})
$$

- 表示节点中最常见类别以外的样本比例
- 最大错分类混杂度=最大Gini混杂度=1-1/n

## 3. 特征选择方法(ID3算法)

ID3算法使用信息增益来选择最佳分割特征：

1. 计算当前节点的熵
2. 对每个特征，计算按该特征分割后的加权平均熵
3. 选择使信息增益(原始熵-分割后熵)最大的特征

**信息增益** = 父节点的熵 - 子节点的加权平均熵

## 4. 决策树构建过程

1. 从根节点开始，所有训练数据都在根节点
2. 选择最佳分割特征(如使用ID3算法)
3. 根据特征值划分数据到子节点
4. 对每个子节点递归执行上述过程，直到：
   - 节点中所有样本属于同一类
   - 没有剩余特征可用于分割
   - 达到预定的停止条件(如树深度限制)

## 5. 决策树的特点

**优点**：

- 模型直观，易于理解和解释
- 能处理数值和类别数据
- 不需要数据归一化
- 可以处理特征间的交互作用

**缺点**：

- 容易过拟合，需要剪枝
- 对数据的小变化可能很敏感
- 可能创建过于复杂的树

## 6. 实际应用示例

文档中提到的"是否可以去享受运动"的例子，决策树可能会考虑以下特征：

- 天气(晴、雨、阴)
- 温度(高、中、低)
- 湿度(高、低)
- 风力(强、弱)

通过构建决策树，可以根据这些特征条件判断某天是否适合户外运动。

决策树学习是机器学习中基础而重要的方法，理解其原理和各种不纯度度量方式对于正确应用和调优模型至关重要。

# 决策树学习进阶

## 1. 决策树过拟合问题

### 1.1 过拟合定义

在决策树中，我们说假设h∈H对训练集过拟合，如果存在另一个假设h'∈H满足：

- 训练误差：err<sub>train</sub>(h) < err<sub>train</sub>(h')
- 测试误差：err<sub>test</sub>(h) > err<sub>test</sub>(h')

### 1.2 极端过拟合案例

- 每个叶节点对应单个训练样本
- 整个树相当于数据查表算法的简单实现
- 训练准确率100%，但泛化性能极差

## 2. 剪枝技术

### 2.1 后剪枝：错误降低剪枝

**步骤**：

1. 将数据集分为训练集和验证集
2. 验证集用途：
   - 已知标签
   - 测试效果
   - 不做模型更新
3. 剪枝过程：
   - 测试剪去每个可能节点(及其子树)的影响
   - 贪心地去掉能提升验证集准确率的节点
   - 直到再剪会损害性能时停止

**叶节点标签确定**：
剪枝后，新叶节点的标签通常设置为该节点中多数样本的类别。

### 2.2 预剪枝 vs 后剪枝

| 比较项 | 预剪枝                  | 后剪枝                  |
| ------ | ----------------------- | ----------------------- |
| 速度   | 更快                    | 较慢                    |
| 准确率 | 一般                    | 更高                    |
| 方法   | 基于样本数/信息增益阈值 | 错误降低剪枝/规则后剪枝 |

## 3. 决策树实际应用扩展

### 3.1 连续属性值处理

- **离散化**：将连续值划分为多个区间
- 常用方法：二分法、等宽分箱、等频分箱

### 3.2 多值属性处理

- 具有过多取值的属性可能导致过拟合
- 解决方法：信息增益比、限制分支数量

### 3.3 缺失值处理

- 忽略含缺失值的样本
- 分配最常用值
- 分配概率值(按已知值的分布)

### 3.4 有代价的属性

- 考虑获取不同属性的代价
- 选择性价比高的分割属性

## 4. 决策树优势与演进

### 4.1 核心优势

- **简单易用**：最常用算法之一
- **易于理解**：类似人类决策过程("如果...就...")
- **实现方便**：计算开销小
- **鲁棒性强**：对噪声数据有抵抗力

### 4.2 算法演进

- **ID3**：基础算法，使用信息增益
- **C4.5**：改进版，处理连续值、缺失值
- **C5.0**：更高效版本
- **决策森林**：由C4.5产生的多棵决策树组合

## 5. 决策树学习总结

### 5.1 基础部分

- 基本概念与ID3算法
- 特征选择与终止条件
- 归纳偏置
- 过拟合问题
- 剪枝技术(预剪枝与后剪枝)

### 5.2 扩展部分

- 连续属性离散化
- 多值属性处理
- 缺失值处理
- 有代价属性考虑

决策树因其直观性和有效性，在实际应用中表现优异。理解这些进阶内容有助于在实际项目中更好地应用和调优决策树模型。

