### **核心概念总览**

| 方面 | L1 (Least Absolute Deviations) | L2 (Least Squares) |
| :--- | :--- | :--- |
| **核心哲学** | 稳健，对异常值不敏感 | 高效，对大幅误差严厉惩罚 |
| **数学形式（范数）** | ‖x‖₁ = Σ\|x_i\| | ‖x‖₂ = √(Σx_i²) (计算时常省略根号，用平方和) |
| **导数** | 不连续 (在0处不可导)，子梯度为 ±1 | 连续可导，导数为 2x_i |

---

## **第一部分：损失函数 (Loss Function)**

损失函数用于衡量模型**预测值 (ŷ)** 与 **真实值 (y)** 之间的误差。它是模型训练过程中需要**最小化**的核心目标。

### **1. L2 损失 (平方损失, MSE)**

*   **公式**:
    `L₂ Loss = Σ(y_i - ŷ_i)²`
    通常使用**均方误差 (MSE)**:
    `MSE = (1/n) * Σ(y_i - ŷ_i)²`

*   **导数**:
    `∂(L₂)/∂ŷ = -2 * (y_i - ŷ_i)`
    梯度与误差成正比。误差越大，梯度越大，参数更新步长越大。

*   **特点**:
    *   **对异常值非常敏感**：由于误差被平方，一个巨大的误差会主导整个损失函数，使模型被迫去拟合异常点，从而降低整体性能。
    *   **具有唯一解**：由于其函数性质，优化过程通常稳定且收敛性好。
    *   **假设误差服从高斯分布**：从最大似然估计的角度看，使用MSE意味着假设数据误差服从高斯分布。

*   **优点**： 数学性质优良，易于计算和求导。
*   **缺点**： 对数据中的异常值（Outliers）鲁棒性差。

### **2. L1 损失 (绝对损失, MAE)**

*   **公式**:
    `L₁ Loss = Σ|y_i - ŷ_i|`
    通常使用**平均绝对误差 (MAE)**:
    `MAE = (1/n) * Σ|y_i - ŷ_i|`

*   **导数**:
    `∂(L₁)/∂ŷ = -1 (if y_i > ŷ_i) or +1 (if y_i < ŷ_i)`
    梯度是常数。无论误差大小，参数更新的步长都相同。

*   **特点**:
    *   **对异常值鲁棒**：误差是线性惩罚，异常点不会获得过高的权重，模型更关注整体趋势。
    *   **解不唯一**：损失函数在0点不可导，优化起来比L2损失更复杂（通常使用次梯度方法）。
    *   **假设误差服从拉普拉斯分布**：从最大似然估计的角度看，使用MAE意味着假设数据误差服从拉普拉斯分布。

*   **优点**： 对异常值不敏感，更稳健。
*   **缺点**： 在0点不可导，优化效率较低。

### **损失函数对比总结**

| 特性 | L2损失 (MSE) | L1损失 (MAE) |
| :--- | :--- | :--- |
| **敏感性** | 对异常值**高敏感** | 对异常值**低敏感（鲁棒）** |
| **梯度性质** | 梯度与误差成正比，更新步长可变 | 梯度为常数，更新步长固定 |
| **解** | **稳定，唯一** | **可能不唯一** |
| **计算效率** | 高，易于优化 | 较低，需特殊优化方法 |
| **适用场景** | 数据干净，噪声小，误差接近高斯分布 | 数据中存在异常值，误差可能重尾 |

---

## **第二部分：正则化 (Regularization)**

正则化项是对模型**权重参数 (w)** 的惩罚项，用于控制模型复杂度，**防止过拟合**。它被加到损失函数中共同构成目标函数。

### **1. L2 正则化 (岭回归, Ridge Regression)**

*   **公式**:
    `L₂ Reg = λ * Σ(w_j²)`
    `λ` 是控制惩罚力度的超参数。

*   **作用机制**:
    *   在损失函数的基础上，附加了一个要求权重向量 **L2范数** 尽可能小的约束。
    *   它倾向于让所有权重都**普遍地、平滑地缩小**，但不会将任何权重恰好变为0。
    *   从几何上看，它将解约束在一个**圆（球）** 内。

*   **影响**:
    *   **降低模型方差**：防止模型过于复杂，提高泛化能力。
    *   **解决共线性问题**：即使特征之间存在高度相关性，L2正则化也能得到一个稳定的解。
    *   **保留所有特征**：所有特征都会被保留在模型中。

*   **别名**: 权重衰减 (Weight Decay)

### **2. L1 正则化 (套索回归, Lasso Regression)**

*   **公式**:
    `L₁ Reg = λ * Σ|w_j|`

*   **作用机制**:
    *   在损失函数的基础上，附加了一个要求权重向量 **L1范数** 尽可能小的约束。
    *   它倾向于产生**稀疏（Sparse）** 的权重矩阵，即它会将一部分**不重要的特征的权重直接压缩至0**。
    *   从几何上看，它将解约束在一个**菱形（多面体）** 内，最优解常出现在菱形的“角”上，使得某些维度的值为0。

*   **影响**:
    *   **自动进行特征选择**：模型会忽略掉不相关的特征，输出一个更简单、可解释性更强的模型。
    *   **降低模型复杂度**：同样可以防止过拟合。

### **正则化对比总结**

| 特性 | L2正则化 (Ridge) | L1正则化 (Lasso) |
| :--- | :--- | :--- |
| **惩罚项** | `λ * Σ(w_j²)` | `λ * Σ|w_j|` |
| **作用** | 收缩权重，**防止过拟合** | 收缩权重并**进行特征选择** |
| **解的性质** | **稠密** (所有权重不为零) | **稀疏** (许多权重为零) |
| **几何约束** | 圆（球）形 | 菱形（多面体） |
| **处理共线性** | 有效，相关特征的权重会变得相近 | 效果差，会随机选择一个特征而将其他相关特征的权重压向0 |
| **别名** | 岭回归、权重衰减 | 套索回归 |

---

## **第三部分：结合使用与综合指南**

### **总目标函数**

机器学习模型的训练过程就是最小化以下总目标函数：

**Total Loss = Loss Function + Regularization Term**

**常见组合：**
1.  **Ridge Regression**: `MSE + L₂ Reg`
2.  **Lasso Regression**: `MSE + L₁ Reg`
3.  **Elastic Net**: `MSE + L₁ Reg + L₂ Reg` (结合两者优点，需调节两个超参数 `λ` 和 `l1_ratio`)
4.  **Robust Model**: `MAE + L₂ Reg` (用于有异常值的回归问题)

### **如何选择？决策指南**

| 你的需求或数据特征 | 推荐方法 | 理由 |
| :--- | :--- | :--- |
| **数据干净，特征不多，只想防止过拟合** | **L2损失 + L2正则 (Ridge)** | 标准配置，稳定可靠。 |
| **数据中有大量异常值** | **L1损失 (MAE) + L2正则** | L1损失对异常值不敏感，保证模型稳健。 |
| **特征数量非常多，想进行特征选择** | **L2损失 + L1正则 (Lasso)** | L1正则能自动筛选出最重要的特征。 |
| **特征高度相关，且数量多于样本数** | **L2损失 + Elastic Net** | L2部分处理共线性，L1部分产生稀疏性，比纯Lasso更稳定。 |
| **追求模型的可解释性，想知道哪些特征最关键** | **L2损失 + L1正则 (Lasso)** | 得到的稀疏权重矩阵直接指出了重要特征。 |

### **超参数 λ 的重要性**

*   `λ` 是控制正则化强度的**超参数**，必须通过交叉验证来调整。
*   `λ = 0`：正则化失效，模型容易过拟合。
*   `λ → ∞`：正则化过强，所有权重被压向0，模型会欠拟合（例如，对于线性模型，只会输出一个常数）。
*   选择一个合适的 `λ` 是在**偏差**和**方差**之间做出最佳权衡的关键。