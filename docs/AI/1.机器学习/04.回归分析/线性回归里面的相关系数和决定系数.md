在统计学和机器学习中，线性回归中的**相关系数（Correlation Coefficient）** 和**决定系数（Coefficient of Determination）** 都是衡量变量间关系强度的指标，但它们侧重点不同。下面详细解释两者的概念、计算、区别和联系：

---

### **1. 相关系数（通常指皮尔逊相关系数 `r`）**
* **定义：** 衡量两个连续变量 `X`（自变量）和 `Y`（因变量）之间**线性关系强度**和**方向**的统计量。
* **范围：** `-1 ≤ r ≤ 1`
* **意义：**
  * `r = 1`：完全正线性相关（所有点落在一条斜率为正的直线上）。
  * `r = -1`：完全负线性相关（所有点落在一条斜率为负的直线上）。
  * `r = 0`：**无线性相关**（但可能存在非线性关系！）。
  * `|r|` 越大，线性关系越强。
* **公式：**
  `r = Σ[(Xi - X̄)(Yi - Ȳ)] / √[Σ(Xi - X̄)² * Σ(Yi - Ȳ)²]`
  * **解读：** 计算的是 `X` 和 `Y` 各自与其均值偏差的乘积之和，再除以它们各自标准差的乘积的平方根。本质上是协方差的标准化。
* **特点：**
  * **对称性：** `r(X, Y) = r(Y, X)`。
  * **无量纲：** 不受 `X` 和 `Y` 测量单位影响。
  * **仅度量线性关系：** 无法捕捉非线性关联。
  * **相关≠因果：** 仅表示统计关联，不能证明 `X` 导致 `Y`。

---

### **2. 决定系数（R² 或 R-squared）**
* **定义：** 衡量回归模型**拟合优度**的统计量。表示模型中**自变量 `X` 能够解释因变量 `Y` 的总变异的比例**。
* **范围：** `0 ≤ R² ≤ 1`（对于简单线性回归）。
* **意义：**
  * `R² = 1`：模型完美拟合数据，所有点都在回归线上（此时 `|r| = 1`）。
  * `R² = 0`：模型完全不能解释 `Y` 的变异（此时 `r = 0`）。
  * `R²` 越接近 1，模型解释能力越强。
* **核心思想（变异分解）：**
  * **总平方和（SST - Total Sum of Squares）：** `Σ(Yi - Ȳ)²` - `Y` 自身总的变异。
  * **回归平方和（SSR - Regression Sum of Squares）：** `Σ(Ŷi - Ȳ)²` - 模型解释的变异（预测值 Ŷ 围绕均值 Ȳ 的波动）。
  * **残差平方和（SSE - Error Sum of Squares）：** `Σ(Yi - Ŷi)²` - 模型未能解释的变异（实际值 `Y` 偏离预测值 Ŷ 的程度）。
  * **关键等式：** `SST = SSR + SSE`
* **公式（基于变异分解）：**
  `R² = SSR / SST = 1 - (SSE / SST)`
  * **解读：**
    * `SSR / SST`：模型解释的变异占总变异的比例。
    * `1 - (SSE / SST)`：总变异中未被模型解释的比例越小，`R²` 越大。
* **特点：**
  * **非对称性：** `R²` 依赖于谁是因变量（`Y`）。
  * **模型评价：** 核心目标是评估模型解释数据变动的能力。
  * **在简单线性回归中的关键联系：** `R² = (r)^2`
    * 简单线性回归（只有一个自变量 `X`）中，决定系数 `R²` 等于皮尔逊相关系数 `r` 的平方。
    * 例如，若 `r = 0.8`，则 `R² = 0.64`，表示 `X` 解释了 `Y` 64% 的变异。
  * **在多元线性回归中：** `R²` 表示**所有**自变量共同解释的 `Y` 变异的比例，此时 `R²` 不再等于某个单一 `r` 的平方（通常报告调整 `R²` 以考虑自变量数量）。

---

### **相关系数（r） vs. 决定系数（R²）核心区别与联系**

| 特性         | 相关系数（r）                | 决定系数（R²）                     |
| :----------- | :--------------------------- | :---------------------------------- |
| **定义**     | 两变量**线性关系强度与方向** | 模型**解释因变量变异比例**          |
| **范围**     | `-1 ≤ r ≤ 1`                 | `0 ≤ R² ≤ 1`（简单线性回归）        |
| **方向**     | 包含正负号（表示方向）       | 无方向（平方值）                    |
| **对称性**   | 对称（`r(X, Y) = r(Y, X)`）  | 非对称（依赖于因变量）              |
| **主要用途** | 描述两变量关联               | 评估回归模型拟合优度                |
| **关系（简单线性回归）** | - | **R² = (r)²** |
| **反映**     | 线性关联强度                 | 模型预测能力（解释变异的比例）      |
| **单位**     | 无量纲                       | 无量纲                              |
| **多元回归** | 通常指单个自变量与 `Y` 的 `r` | 所有自变量共同解释的变异比例        |

---

### **重要注意事项**

1. **`R²` 高不一定好：**
   * 过拟合：添加无关变量总会略微增加 `R²`（即使无真实关系），此时需看**调整 `R²`**。
   * 不代表因果关系。
   * 可能受异常值影响。
2. **`r = 0` 不代表无关系：** 只表示无*线性*关系，可能存在曲线关系（如 `U` 型）。
3. **`R²` 与 `r` 的关系仅在简单线性回归中成立：** 多元回归中 `R²` 反映整体解释力，不等于某个 `r` 的平方。
4. **选择：**
   * 想了解两个变量间线性关联的**强度和方向**？ → 用 **相关系数 `r`**。
   * 想评估建立的（线性）回归模型**解释数据变异的能力**？ → 用 **决定系数 `R²`**。

---

**总结：**

* **相关系数 `r`** 是你理解 `X` 和 `Y` 之间线性纽带有多紧密（及正负）的起点。
* **决定系数 `R²`** 则是当你用 `X`（通过线性模型）去预测 `Y` 时，它能告诉你这个模型到底“抓住”了 `Y` 的多少变化规律。
* 在只有一个自变量的简单线性回归模型中，`R²` 就是 `r` 的平方，这是两者最直接的联系。但在更复杂的模型中，`R²` 承载了更丰富的模型评价信息。