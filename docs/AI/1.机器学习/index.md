## 机器学习核心框架



### **一、机器学习本质与定义**

#### **核心框架（TEP）**

- **任务（Task）**：目标场景（如下棋、图像识别）  
- **经验（Experience）**：训练数据（如历史棋局、标注数据集）  
- **性能（Performance）**：评价指标（如胜率、准确率）  

> *Tom Mitchell定义：系统通过经验在任务上提升性能*

#### **经典观点**

- **Herbert Simon**：学习是系统通过实践提升效率的过程  
- **Ryszard Michalski**：学习是对经验表示方法的构造与修改  

---

### **二、机器学习系统设计五步法**（以国际跳棋AI为例）

#### **1. 经验设计：训练数据的代表性与偏差**

- **关键问题**：经验能否反映真实场景？  
  - *案例1*：自动驾驶训练数据缺少雪天场景 → 实际路测失败  
  - *案例2*：IBM Watson误判“多伦多属美国城市”（训练数据缺乏地理歧义样本）  
- **解决方案**：  
  - 数据多样性（如自动驾驶需覆盖雪地/山地/城市等场景）  
  - 平衡数据分布（避免月亮与香蕉分类器因样本偏差误判）  

#### **2. 目标函数：定义学习目标**

- **理想目标**：完美价值函数 \( V(b) \)（b=棋盘状态）  

  - \( V(b)=100 \)（胜），\( V(b)=-100 \)（败），\( V(b)=0 \)（平）  

- **现实妥协**：直接计算 \( V(b) \) 不可行 → 需近似假设 \( \hat{V}(b) \)  

  > 例如：用后续最优状态 \( V(b') \) 逼近 \( V(b) \)

#### **3. 假设表示：选择模型结构**

- **可选表示方法**：  

  | **类型** | **案例**             | **特点**             |
  | -------- | -------------------- | -------------------- |
  | 状态表   | 穷举所有棋盘状态     | 精确但存储爆炸       |
  | 线性模型 | 加权特征求和（如下） | 高效但表达能力有限   |
  | 神经网络 | 多层感知机           | 强拟合能力需大量数据 |

- **跳棋特征设计**：  
  \[
  \hat{V}(b) = w_0 + w_1 \cdot \text{wp}(b) + w_2 \cdot \text{rp}(b) + \cdots + w_6 \cdot \text{rt}(b)
  \]  

  > `wp`=白棋数, `rp`=红棋数, `wt`=白棋受威胁数...

#### **4. 学习算法：优化模型参数**

- **目标**：最小化预测误差 \( \sum (V_{\text{train}}(b) - \hat{V}(b))^2 \)  

- **梯度下降流程**：  

  ```python
  初始化权重 w = [w0, w1, ..., w6]
  for each 训练样本 b:
      error = V_train(b) - ̂V(b)          # 计算误差
      for each 权重 w_i:
          w_i ← w_i + η · f_i · error    # η=学习率（如0.1）, f_i=特征值
  ```

以下是**闭环迭代：自我博弈优化**的简化描述（以国际跳棋AI为例）：

---

#### **5.三步循环流程**

1. **初始模型**  
   随机初始化价值预测模型 \(\hat{V}\)（例如：给棋子数、威胁数等特征赋随机权重）。

2. **自我对弈生成数据**  
   - 让当前模型 \(\hat{V}\) 自己和自己下棋，记录每一步的棋盘状态序列。  
   - 

3. **标注数据 & 更新模型**  
   - **标注**：用后续状态的价值标注当前状态：  
   - **训练**：用梯度下降法更新 \(\hat{V}\) 的权重，最小化预测误差：  

---

### **为何能持续优化？**

- **数据迭代升级**：模型越强→生成的棋局质量越高→训练数据更接近高手对局。  
- **目标渐进逼近**：每次用 \(\hat{V}(b_{i+1})\) 标注 \(b_i\)，本质是让当前状态价值向后续最优状态对齐，逐步逼近真实 \(V(b)\)。  
- **无需人类干预**：系统自主产生数据，摆脱对人工标注的依赖。

> **类比**：如同棋手通过不断复盘自己的对局（左手vs右手）发现漏洞、调整策略，最终越练越强。

> *通过不断生成新棋局，模型持续逼近真实 \( V(b) \)*

---

### **三、机器学习基础概念体系**

#### **1. 核心组件**

| **概念**         | **定义**                                       | **实例**                   |
| ---------------- | ---------------------------------------------- | -------------------------- |
| 实例空间 \( X \) | 所有可能输入集合                               | 棋盘所有状态（约10²⁰种）   |
| 假设空间 \( H \) | 候选模型的集合                                 | 线性函数/决策树/神经网络等 |
| 目标概念 \( C \) | 待学习的真实映射函数                           | 完美价值函数 \( V(b) \)    |
| 训练集 \( D \)   | 标注样本集合 \( \langle x_i, c(x_i) \rangle \) | 带胜负标签的棋盘状态       |

#### **2. 关键挑战**

- **假设空间复杂度**：  

  - \( n \)个二值特征 → 假设空间 \( |H| = 2^{2^n} \)（*指数爆炸*）  

  > *例：20个特征时 \( |H| \approx 10^{300000} \)，无法遍历*

- **近似必然性**：  

  - 追求 \( h(x) = c(x) \) 对所有 \( x \in X \) 不可行 → 优化 \( h(x) = c(x) \) 在 \( D \) 上的表现  

---

### **四、应用场景与设计选择关联**

| **应用场景** | **经验设计**     | **目标函数** | **表示选择**      |
| ------------ | ---------------- | ------------ | ----------------- |
| 电商推荐系统 | 用户历史行为日志 | 点击率预测   | 矩阵分解/神经网络 |
| 自动驾驶     | 多气候路测数据   | 安全驾驶策略 | CNN+强化学习      |
| 信用风险评估 | 客户历史还款记录 | 违约概率     | 逻辑回归/决策树   |

