本实验旨在实现多层感知机(MLP)对MNIST手写数字数据集进行分类识别，通过对比不同激活函数(Sigmoid、ReLU)和损失函数(欧式距离、交叉熵)的组合，以及单隐含层与双隐含层网络结构，分析各种配置对模型性能的影响。

在完成代码之后发现
- 最好的是 ReLU+交叉熵: 14.76%
- 双隐含层反而更差: 6.34%
- 其他都在10%左右（接近随机猜测）
后续调整了


### 基础网络结构

- **输入层：** 784个神经元(28×28像素)
- **隐含层：** 128个神经元(单隐含层)或256→128个神经元(双隐含层)
- **输出层：** 10个神经元(对应10个类别)

### 超参数
```python
batch_size = 64 
max_epoch = 20 
init_std = 0.01 
learning_rate_SGD = 0.01 
weight_decay = 0.0001
```
### 实验设计
#### 对比实验组合

1. **Sigmoid + 欧式距离损失**
2. **ReLU + 欧式距离损失**
3. **Sigmoid + 交叉熵损失**
4. **ReLU + 交叉熵损失**
5. **双隐含层ReLU + 交叉熵损失**

#### 训练流程

- 数据标准化处理
- 批量梯度下降训练
- 每个epoch后进行验证
- 记录训练和验证的损失及准确率
其中绘制的每一个实验组合的**Losses**和**Accuracies**图包含了
- 浅色线：所有batch的训练损失 (train all) - 显示训练过程的详细波动
- 蓝色点线：每个epoch的平均训练损失 (train)
- 橙色点线：验证损失 (val)
- 其中**Sigmoid + 欧式距离损失**
## 性能总结

### 各模型测试准确率对比

| 模型配置           | 测试准确率  |
| -------------- | ------ |
| Sigmoid + 欧式距离 | 88.86% |
| ReLU + 欧式距离    | 91.40% |
| Sigmoid + 交叉熵  | 90.29% |
| ReLU + 交叉熵     | 93.12% |
| 双隐含层ReLU + 交叉熵 | 93.84% |
其中发现:
	• ReLU激活函数优于Sigmoid激活函数
	• 交叉熵损失函数优于欧式距离损失函数
	• 双隐含层网络性能最佳
	• 最佳组合准确率: 93.84%


### 激活函数对比分析

**ReLU vs Sigmoid：**

- 使用欧式距离损失时：ReLU(91.40%) > Sigmoid(88.86%)，提升2.54%
- 使用交叉熵损失时：ReLU(93.12%) > Sigmoid(90.29%)，提升2.83%

**结论：** ReLU激活函数在所有组合下均优于Sigmoid激活函数。

### 损失函数对比分析

**交叉熵 vs 欧式距离：**

- 使用Sigmoid激活时：交叉熵(90.29%) > 欧式距离(88.86%)，提升1.43%
- 使用ReLU激活时：交叉熵(93.12%) > 欧式距离(91.40%)，提升1.72%

**结论：** 交叉熵损失函数在所有组合下均优于欧式距离损失函数。

### 网络深度影响

**双隐含层 vs 单隐含层：**

- 单隐含层(128个神经元)：93.12%
- 双隐含层(256→128个神经元)：93.84%
- 性能提升：0.72%

