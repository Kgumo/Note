包括俩个，一个是**PyTorch 模型转换为 ONNX 格式**，一个是**C++ 中使用 ONNX Runtime 进行推理**。
可能用到的github的官方教程
[onnx/tutorials：创建和使用 ONNX 模型的教程](https://github.com/onnx/tutorials)
[使用 ONNX 运行时进行机器学习推理的示例。](https://github.com/microsoft/onnxruntime-inference-examples)
Netron 支持 ONNX、TensorFlow Lite、Core ML、Keras、Caffe、Darknet、PyTorch、TensorFlow.js、Safetensors 和 NumPy
[lutzroeder/netron：神经网络、深度学习和机器学习模型的可视化工具](https://github.com/lutzroeder/Netron)
[zetane/viewer：ML 模型和内部张量 3D 可视化工具](https://github.com/zetane/viewer?tab=readme-ov-file)

---
## 转换
### 工作流程
1. **准备阶段**：获得 `.pt` 文件和了解模型结构
    
2. **转换阶段**：使用 PyTorch 的 `torch.onnx.export()` 转换为 `.onnx`
    
3. **验证阶段**：确保转换后的模型能正常推理
    
4. **部署阶段**：使用我之前提供的 C++ ONNX Runtime 代码进行部署
### **转换前的准备工作：**

1. **了解模型结构**：你需要知道原始 PyTorch 模型的定义
    
2. **输入形状**：明确模型期望的输入张量形状
    
3. **模型类型**：确认是只有权重还是包含完整结构
### **如果不知道模型结构：**
```python
# 可以尝试这样查看 .pt 文件内容
checkpoint = torch.load('your_model.pt', map_location='cpu')
print("Keys in checkpoint:", checkpoint.keys())

# 如果是包含结构的完整模型
if 'model_state_dict' in checkpoint:
    print("Model state dict keys:", checkpoint['model_state_dict'].keys())
```

### 1. **准备 Python 转换环境**
```bash
pip install torch onnx onnxruntime
```
### 2. **编写转换脚本**
大致如下(例如 `convert_pt_to_onnx.py`)：
```python
import torch
import torch.onnx
import onnx

# 加载你的 .pt 文件
# 方法1: 如果只有模型权重，需要先定义模型结构
class YourModelClass(torch.nn.Module):
    def __init__(self):
        super(YourModelClass, self).__init__()
        # 这里定义你的模型层结构
        # 需要与训练时的模型结构完全一致
        
    def forward(self, x):
        # 定义前向传播
        return x

model = YourModelClass()
model.load_state_dict(torch.load('your_model.pt'))
model.eval()

# 方法2: 如果 .pt 文件包含完整模型（包括结构）
# model = torch.load('your_model.pt')
# model.eval()

# 创建示例输入（重要：需要与模型期望的输入形状一致）
# 根据你的模型调整 batch_size, channels, height, width
dummy_input = torch.randn(1, 3, 224, 224)  # 示例：batch=1, 3通道, 224x224图像

# 设置输入输出名称（在C++中会用到）
input_names = ["input"]
output_names = ["output"]

# 导出为ONNX
torch.onnx.export(
    model,
    dummy_input,
    "converted_model.onnx",  # 输出的ONNX文件名
    export_params=True,      # 存储训练好的参数
    opset_version=11,        # ONNX算子集版本
    input_names=input_names, # 输入节点名称
    output_names=output_names, # 输出节点名称
    dynamic_axes={
        'input': {0: 'batch_size'},    # 动态批次维度
        'output': {0: 'batch_size'}
    } if needs_dynamic_axes else None  # 如果需要动态形状
)

print("转换成功！模型已保存为 converted_model.onnx")
```
PyTorch官方给的
```python
import torch

class MyModel(torch.nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 128, 5)

    def forward(self, x):
        return torch.relu(self.conv1(x))

input_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)

model = MyModel()

torch.onnx.export(
    model,                  # model to export
    (input_tensor,),        # inputs of the model,
    "my_model.onnx",        # filename of the ONNX model
    input_names=["input"],  # Rename inputs for the ONNX model
    dynamo=True             # True or False to select the exporter to use
)
```
### 3. **运行转换脚本**
```bash
python convert_pt_to_onnx.py
```
---
## C++先决条件
首先，确保你的开发环境已就绪，并建议按以下结构组织项目文件夹
##### **编译环境**：
[**ONNX Runtime包**](https://github.com/RapidAI/OnnxruntimeBuilder/releases)
VSCode需安装C++扩展和CMake扩展。
**ONNX Runtime库**：从预编译的**ONNX Runtime包** ，将其中的 **`include`** 文件夹和GPU版本的库文件（例如 `onnxruntime.lib` 及运行时依赖 `onnxruntime.dll`）准备好。GPU版本通常需要匹配的CUDA和cuDNN环境。
- **模型文件**：将你的 `.onnx` 模型文件放在项目合适的位置，例如示例中的 `models/` 目录。
##### 文件夹参考：
```text
YourProjectName/
├── CMakeLists.txt
├── src/
│   └── main.cpp
├── include/
├── models/
│   └── your_model.onnx    // 你的ONNX模型文件
└── third_party/
    └── onnxruntime/
        ├── include/       // ONNX Runtime头文件
        └── lib/           // ONNX Runtime库文件
```
---
### 配置 VSCode 与 CMake
在项目根目录创建 **`CMakeLists.txt`** 文件，这是编译组织的关键
其中我一般考虑编译生成的可执行文件放在在 `build` 目录下。
	如果在CMakeLists.txt文件所在目录执行了cmake命令之后就会生成一些目录和文件（包括 makefile 文件），如果再基于makefile文件执行make命令，程序在编译过程中还会生成一些中间文件和一个可执行文件，这样会导致整个项目目录看起来很混乱，不太容易管理和维护，此时我们就可以把生成的这些与项目源码无关的文件统一放到一个对应的目录里边，比如将这个目录命名为build
所以
```shell
mkdir build
cd build
cmake ..
```

现在cmake命令是在build目录中执行的，但是CMakeLists.txt文件是build目录的上一级目录中，所以cmake 命令后指定的路径为..，即当前目录的上一级目录。

当命令执行完毕之后，在build目录中会生成一个makefile文件

---
### 编写基础推理代码

编写代码
`#include <onnxruntime_cxx_api.h> // ONNX Runtime C++ API:cite[3]:cite[10]`
#### 运行你的程序

1. **直接运行**：如果编译成功，在VSCode集成终端中，进入你的 `build` 目录，直接运行生成的可执行文件。
    
2. **确保动态库可用**：运行时，需要确保 `onnxruntime.dll` 在系统路径中，或者将其拷贝到你的可执行文件同一目录下。对于GPU版本，还需确保CUDA和cuDNN的DLL文件可用

### 其他
- **模型输入输出**：务必根据你的ONNX模型调整代码中的输入输出名称、形状和数据类型。可以使用 [Netron](https://github.com/lutzroeder/netron) 可视化模型来确认这些信息。
    
- **GPU支持**：要使用GPU推理，除了链接GPU版本的ONNX Runtime库，C++代码中配置SessionOptions时通常需要显式添加CUDA执行提供器。具体API请查阅ONNX Runtime的官方C++文档。
    
- **错误排查**：编译或运行时遇到问题，请仔细检查：
    
    - 头文件和库路径是否正确。
        
    - 链接的库文件名是否精确。
        
    - 模型路径是否正确。
        
    - 运行时的所有必要动态库（如 `onnxruntime.dll`、CUDA相关DLL）是否可用。