## 总体
- 目标：用 ONNX Runtime 在 C++ 侧对一个 CNN-CIFAR-10 模型进行推理，支持 CPU/GPU（CUDA）执行提供器，并对测试集进行端到端的推理、统计与性能评估。
- 关键能力点：初始化 ONNX Runtime 环境、从内存加载模型、获取输入/输出信息、对 CIFAR-10 数据进行输入格式化、执行推理并计时、对结果进行后处理和性能评估。
- 典型工作流：从 CIFAR-10 Reader 获得归一化的输入向量（1x3x32x32 的 float 输入），通过 predict_with_time 进行推理，随后 postprocess_results 打印预测结果并更新统计；在 main 中串联完整的测试与基准流程。

#### 1. 主要类与核心职责

- CIFAR10Classifier：一个封装好的推理管线
    - 成员变量
        - env、session：ONNX Runtime 的环境与会话对象，负责模型推理的全局上下文与会话状态。
        - input_names_str、output_names_str、input_names、output_names：用于记录模型输入输出节点名称，便于 Run 时提供正确的输入输出句柄。
        - input_shape：输入张量的形状信息，假设模型输入为 1x3x32x32 的四维张量。
        - use_gpu：是否使用 GPU（CUDA）执行提供器。
        - 统计相关变量：total_tested、total_correct、total_inference_time，用于测试过程中的计数与时间统计。
    - 构造函数的职责
        - 初始化 ONNX Runtime 环境，尝试开启 CUDA（如果 requested），并打印可用的执行提供器。
        - 验证模型文件完整性（存在、非空），以便后续加载。
        - 设置会话选项（线程数、图优化等）。
        - 从内存加载模型并创建会话（loadModelFromMemory）。
        - 获取输入/输出节点信息、输入形状和名称，打印以便调试。
    - 预测相关方法
        - predict_with_time：给定一个 float 输入向量，构造输入张量、执行推理并返回输出向量以及推理耗时（微秒）。
        - postprocess_results：对推理结果进行简单的后处理（找出最大概率的类别、对比真实标签、输出前三个候选等），并返回是否正确的标志。
    - 测试与基准方法
        - test_with_real_data：加载 CIFAR-10 测试集数据，逐张图像进行推理并统计正确率、耗时等。
        - display_final_results：输出最终统计信息（准确率、平均推理时间、FPS 等）。
        - benchmark_performance：对单张测试样本进行多次推理的性能基准，给出平均推理时间与 FPS。
    - 资源管理
        - 析构函数输出提示，表示资源清理。

#### 2. 数据流与推理流程的设计动机

- 数据来源与输入格式
    - 数据来自 CIFAR-10Reader，读取后得到每张图像的归一化像素向量 image.data，长度应为 3072，对应 32x32 的 3 通道数据，按照 CIFAR-10 的通道顺序（R 1024, G 1024, B 1024）排列。
    - 预测输入形状设定为 {1, 3, 32, 32}，与常见的 NCHW 模型输入一致，数据存放顺序与 1x3x32x32 的张量内存布局相符合。
- 模型加载与执行提供器
    - 构造函数中对 CUDA 进行尝试启用，若失败则回退到 CPU。这样设计的动机是：在有 NVIDIA GPU 的环境中优先利用 GPU 加速，在无 GPU 或驱动/CU DNN 不可用时仍能工作。
    - 加载模型的实现选择“从内存加载模型”是为了演示在内存中持有模型数据并创建 ONNX Runtime 会话的能力，理论上可以减少磁盘 I/O 次数，便于在某些动态模型加载场景中使用。
- 输入/输出信息的获取
    - 构造阶段读取并保存输入、输出节点名称，以及输入形状，并在控制台打印，便于调试推理管线是否正确对齐。
    - 采用单输入、单输出的简化场景，符合典型的分类卷积网络结构。
- 推理与统计
    - 通过 predict_with_time 进行单次推理，返回输出向量与推理耗时，后续再进行 postprocess_results。
    - test_with_real_data 会逐张图像推理并累计正确率和耗时，display_final_results 给出综合评估。
    - benchmark_performance 提供一个简单的性能基准，便于比较不同输入情况下的吞吐与延迟。

#### 3. 设计与实现

- CUDA 提供器切换与日志信息
    - 通过 try-catch 机制尝试追加 CUDA 执行提供器，成功后打印可用提供器列表，失败则回退 CPU。
    - 这是一个对开发者友好的方式：不强制依赖 GPU，而是在可用时就启用。
- 内存模型加载的演示
    - loadModelFromMemory 将模型文件读入内存后用 Ort::Session 构造函数在内存中建立会话，以演示“内存加载”的能力。
    - 现实场景中要注意模型字节流的生命周期，确保模型数据在整個会话生命周期内有效。
- 性能统计与输出
    - 通过 format_duration 将微秒、毫秒、秒等格式化输出，便于报告和调试。
    - test_with_real_data 与 display_final_results 提供清晰的准确率、推理时间、FPS 等指标，便于评估模型在实际推理中的表现。
- 流程可重复性与基准
    - benchmark_performance 提供一个简单的重复性基准入口，方便在相同环境下多轮对比。

#### 4. 需要注意的潜在问题与风险点

- 模型数据的生命周期（核心风险1）
    - loadModelFromMemory 将模型数据存放在局部的 `std::vector<char> model_data` 中，然后把 model_data.data() 传给 Ort::Session。离开该函数后，model_data 会被析构，导致会话中持有的模型数据指针指向已释放的内存。
    - 结论：模型字节流需要在整个 CIFAR10Classifier 对象的生命周期内有效。通常做法是将模型字节流作为类的成员变量（如 `std::vector<char> model_bytes_;`），确保在类实例销毁前模型数据仍然有效。你当前的实现可能在推理阶段因为模型数据内存无效而崩溃。
- 输入/输出名称与内存生命周期（核心风险2）
    - 通过 GetInputNameAllocated(i, allocator) 获取输入名并存储到 input_names_str，随后把 input_names 指向 input_names_str.back().c_str()。若 input_names_str 在此过程中发生重新分配，之前存放的指针会失效，导致 Run 时访问非法内存。
    - 另外，allocator 是在构造函数中的一个局部对象，构造结束后就会被销毁，若 ONNX Runtime 需要通过 allocator 管理的内存，内存也会失效。
    - 解决思路（不在本次改动范围内详述代码，仅说明设计层面）：将 allocator 作为类的成员变量来维持整个对象寿命；将 input 名称的存储设计成在输入阶段就一次性获取并存放在一个稳定的容器中（避免在运行时再通过可能被重新分配的字符串指针引用名称）。
- 单输入/单输出的假设
    - 代码假设只有一个输入和一个输出。若模型包含多个输入输出（例如多分支的网络、辅助输入等），需要改造 input_names/output_names 的管理逻辑，确保所有输入输出都被正确传递和读取。
- 硬编码路径与平台耦合
    - main() 中模型路径是硬编码的 Windows 风格路径，这会降低在其他平台或不同工作目录下的可移植性。实际部署时应考虑将路径参数化或通过配置文件传入。
- from-memory 加载的实际可用性
    - RETRO：不同 ONNX Runtime 版本对从内存加载模型的行为可能略有差异（尤其是内存生命周期、对齐要求等）。在实际环境中请确保与你的 ONNX Runtime 版本兼容。
- 资源与多线程
    - session_options 设置了 IntraOp/InterOp 线程数为 1，确保单线程执行以简化调试，但在性能对比时要注意这会限制并行性。若真正需要并行，可以考虑合适地调整线程数。
- 依赖与环境
    - CUDA 提供器需要正确的 CUDA/cuDNN 版本与驱动，且 ONNX Runtime 需要编译时对 CUDA 的支持。若环境不具备 CUDA，会回退到 CPU，需确保在 CPU 模式下仍然能正确推理。

#### 5. 设计动机回顾

- GPU/CPU 双模式的选择体现对通用性与鲁棒性的追求：在有 GPU 的开发环境中优先利用硬件加速，否则保留 CPU 跑通，避免整段流程因为环境问题而卡死。
- 从内存加载模型的演示，强调数据流在不同来源（磁盘、内存）上的灵活性，方便后续将模型部署到更复杂的应用场景（如热加载/热更新）。
- 详细的性能统计与测试流程，帮助对比不同输入数据、不同执行提供器下的性能差异，便于在实际部署前做基线评估。
- 通过 CIFAR-10 Reader 与推理管线的组合，形成一个端到端的示例，方便你在笔记或文档中直接复现实验。

### 类成员变量设计
```C++
private:
    Ort::Env env;                             // ONNX Runtime环境
    Ort::Session session;                     // ONNX模型会话
    std::vector<std::string> input_names_str; // 输入节点名称字符串存储
    std::vector<std::string> output_names_str; // 输出节点名称字符串存储
    std::vector<const char*> input_names;     // 输入节点名称指针数组
    std::vector<const char*> output_names;    // 输出节点名称指针数组
    std::vector<int64_t> input_shape;         // 输入张量形状
    bool use_gpu;                             // GPU使用标志
    int total_tested = 0;                     // 统计总测试数量
    int total_correct = 0;                    // 统计正确预测数量
    long long total_inference_time = 0;       // 统计总推理时间
```
**设计思路：**

- **双重名称存储**：`input_names_str`存储字符串对象，`input_names`存储C风格字符串指针，满足ONNX Runtime API要求
- **统计信息**：实时记录测试进度和性能指标，便于最终分析
- **GPU标志**：动态控制执行提供器选择
## 核心组件解析

### 1. 环境初始化与模型加载

```C++
CIFAR10Classifier(const std::string& model_path, bool use_cuda = false)
    : env(ORT_LOGGING_LEVEL_WARNING, "CIFAR10"), // 创建ONNX环境
      session(nullptr),
      use_gpu(use_cuda)
{
    // 验证模型文件完整性
    validateModelFile(model_path);
    
    // 配置会话选项
    Ort::SessionOptions session_options;
    session_options.SetIntraOpNumThreads(1);
    session_options.SetInterOpNumThreads(1);
    
    // GPU配置尝试
    if(use_cuda){
        try{
            Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0));
            // 显示可用提供器
            auto providers = Ort::GetAvailableProviders();
        }catch(const std::exception& e){
            use_gpu = false; // 失败时回退到CPU
        }
    }
}
```
**设计思路：**

- **渐进式初始化**：先验证文件，再配置会话，最后加载模型
- **容错机制**：GPU初始化失败时自动回退到CPU，确保程序可用性
- **信息展示**：显示可用执行提供器，便于调试
### 2. 模型文件验证与内存加载

```C++
void validateModelFile(const std::string& model_path) {
    std::ifstream file(model_path, std::ios::binary);
    if(!file.is_open()){
        throw std::runtime_error("无法打开模型文件: " + model_path);
    }
    
    file.seekg(0, std::ios::end);
    size_t file_size = file.tellg();
    file.seekg(0, std::ios::beg);
    
    if(file_size == 0){
        throw std::runtime_error("模型文件为空: " + model_path);
    }
}
```
**设计思路：**

- **前置验证**：在模型加载前检查文件存在性和完整性
- **详细错误信息**：提供具体的文件路径和大小信息，便于问题定位
### 3. 推理执行核心

```C++
std::pair<std::vector<float>, long long> predict_with_time(const std::vector<float>& input_data){
    // 验证输入数据大小
    size_t expected_size = 1 * 3 * 32 * 32; // 1张图像, 3通道, 32x32分辨率
    if(input_data.size() != expected_size){
        throw std::runtime_error("输入数据大小不匹配");
    }
    
    // 创建输入张量
    std::vector<int64_t> actual_input_shape = {1, 3, 32, 32};
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info,
        const_cast<float*>(input_data.data()),
        input_data.size(),
        actual_input_shape.data(),
        actual_input_shape.size()
    );
    
    // 计时推理
    auto start_time = std::chrono::high_resolution_clock::now();
    auto output_tensors = session.Run(
        Ort::RunOptions{nullptr},
        input_names.data(),
        &input_tensor,
        1,
        output_names.data(),
        1
    );
    auto end_time = std::chrono::high_resolution_clock::now();
    
    // 返回结果和推理时间
    return std::make_pair(output_vector, duration.count());
}
```
**设计思路：**

- **精确计时**：使用高精度时钟测量纯推理时间
- **输入验证**：确保输入数据格式与模型期望完全匹配
- **返回值设计**：同时返回推理结果和耗时，便于性能分析
### 4. 结果后处理与统计

```C++
int postprocess_results(const std::vector<float>& predictions, int true_label, long long inference_time){
    // 找到概率最高的类别
    int predicted_class = 0;
    float best_score = predictions[0];
    for(size_t i = 1; i < predictions.size(); i++){
        if(predictions[i] > best_score){
            best_score = predictions[i];
            predicted_class = static_cast<int>(i);
        }
    }
    
    // 显示详细信息
    auto class_names = CIFAR10Reader::get_class_names();
    std::cout << "预测类别:" << predicted_class << "(" << class_names[predicted_class] << ")" << std::endl;
    std::cout << "真实类别:" << true_label << "(" << class_names[true_label] << ")" << std::endl;
    
    // 返回正确性标志
    return (predicted_class == true_label) ? 1 : 0;
}
```
**设计思路：**

- **直观显示**：同时显示数字标签和类别名称，便于理解
- **置信度展示**：显示预测的置信度分数
- **错误分析**：对错误预测显示前3个可能类别，便于分析模型决策

### 5. 批量测试与性能统计
```C++
void test_with_real_data(int num_test_images = -1){
    auto test_images = CIFAR10Reader::read_test_data();
    
    // 进度显示
    int progress_interval = std::max(1, num_test / 10);
    
    // 记录总时间
    auto total_start_time = std::chrono::high_resolution_clock::now();
    
    for(int i = 0; i < num_test; ++i){
        // 进度更新
        if(i % progress_interval == 0) {
            std::cout << "进度: " << (i + 1) << "/" << num_test << std::endl;
        }
        
        // 执行推理并统计
        auto result = predict_with_time(image.data);
        total_inference_time += result.second;
        total_tested++;
        total_correct += postprocess_results(result.first, image.label, result.second);
    }
    
    // 显示最终统计
    display_final_results(total_duration.count());
}
```
**设计思路：**

- **灵活测试**：支持测试指定数量或全部图像
- **进度反馈**：定期显示测试进度，避免长时间无响应
- **全面统计**：记录准确率、时间效率等多维度指标
### 6. 性能分析与报告

```C++
void display_final_results(long long total_duration_us) {
    // 基础统计
    float accuracy = static_cast<float>(total_correct) / total_tested * 100.0f;
    
    // 时间效率统计
    double avg_inference_time = static_cast<double>(total_inference_time) / total_tested;
    double inference_fps = 1000000.0 / avg_inference_time;
    
    // 性能评级
    if(accuracy >= 90.0f) {
        std::cout << "  [优秀] 准确率极高" << std::endl;
    }
    // ... 其他评级
    
    // GPU加速效果评估
    if(use_gpu && avg_inference_time < 5000) {
        std::cout << "  GPU加速效果: [显著]" << std::endl;
    }
}
```
**设计思路：**

- **多维度分析**：准确率、速度、GPU加速效果全面评估
- **直观评级**：根据准确率给出性能评级
- **实用建议**：基于测试结果给出改进建议

## 代码
```C++
#include <iostream>

#include <vector>

#include <fstream>

#include <filesystem>

#include <sstream>  // 添加stringstream头文件

#include <iomanip>  // 用于时间格式化

#include <onnxruntime_cxx_api.h>  // 1. 头文件引入 - ONNX Runtime核心API

#include "cifar10_reader.h"

#include <chrono>

#include <memory>

#include <algorithm>

  

class CIFAR10Classifier {

private:

    Ort::Env env;                             // 2. 环境初始化 - ONNX Runtime环境

    Ort::Session session;                     // 3. 会话创建 - ONNX模型会话

    std::vector<std::string> input_names_str; // 4. 输入准备 - 输入节点名称字符串存储

    std::vector<std::string> output_names_str; // 6. 输出处理 - 输出节点名称字符串存储

    std::vector<const char*> input_names;     // 4. 输入准备 - 输入节点名称指针数组

    std::vector<const char*> output_names;    // 6. 输出处理 - 输出节点名称指针数组

    std::vector<int64_t> input_shape;         // 4. 输入准备 - 输入张量形状

    bool use_gpu;                             // 2. 环境初始化 - GPU使用标志

    int total_tested = 0;                     // 统计总测试数量

    int total_correct = 0;                    // 统计正确预测数量

    long long total_inference_time = 0;       // 统计总推理时间（微秒）

  

public:

    CIFAR10Classifier(const std::string& model_path, bool use_cuda = false)

    : env(ORT_LOGGING_LEVEL_WARNING, "CIFAR10"), // 2. 环境初始化 - 创建ONNX环境

      session(nullptr),

      use_gpu(use_cuda)

    {

        std::cout << "1. 初始化ONNX Runtime环境..." << std::endl;

        // 3. 会话创建 - 验证模型文件完整性（新功能）

        std::cout << "验证模型文件完整性..." << std::endl;

        validateModelFile(model_path);

        // 3. 会话创建 - 配置会话选项

        Ort::SessionOptions session_options;

        session_options.SetIntraOpNumThreads(1);  // 设置线程数

        session_options.SetInterOpNumThreads(1);

        // 2. 环境初始化 - 配置执行提供器 (CPU/GPU)

        if(use_cuda){

            try{

                std::cout << "尝试启用CUDA执行提供器..." << std::endl;

                // 使用CUDA执行提供器（新功能）

                Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0));

                std::cout << "[OK] CUDA执行提供器启用成功!" << std::endl;  // 替换Unicode字符

                // 显示可用的执行提供器（新功能）

                std::cout << "可用执行提供器:" << std::endl;

                auto providers = Ort::GetAvailableProviders();

                for(const auto& provider : providers){

                    std::cout << "  - " << provider << std::endl;

                }

            }catch(const std::exception& e){

                std::cerr << "[FAIL] CUDA执行提供器启用失败: " << e.what() << std::endl;  // 替换Unicode字符

                std::cerr << "[FAIL] 回退到CPU执行提供器" << std::endl;  // 替换Unicode字符

                use_gpu = false;

            }

        }else{

            std::cout << "[OK] 使用CPU执行提供器" << std::endl;  // 替换Unicode字符

            use_gpu = false;

        }

        // 3. 会话创建 - 设置图优化级别

        session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);

        std::cout << "2. 加载模型:" << model_path << std::endl;

        try{

            // 3. 会话创建 - 从内存加载模型以检查兼容性（新功能）

            loadModelFromMemory(model_path, session_options);

        }catch(const std::exception& e){

            throw std::runtime_error("[FAIL] 错误加载模型: " + std::string(e.what()));  // 替换Unicode字符

        }

        // 4. 输入准备 - 获取模型输入输出信息

        Ort::AllocatorWithDefaultOptions allocator;

  

        size_t num_input_nodes = session.GetInputCount();

        size_t num_output_nodes = session.GetOutputCount();

        std::cout << "输入节点数量: " << num_input_nodes << std::endl;

        std::cout << "输出节点数量: " << num_output_nodes << std::endl;

  

        // 4. 输入准备 - 获取输入节点名称和形状

        for(size_t i = 0; i < num_input_nodes; i++) {

            auto input_name_ptr = session.GetInputNameAllocated(i, allocator);

            std::string input_name = input_name_ptr.get();

            input_names_str.push_back(input_name);

            input_names.push_back(input_names_str.back().c_str());

            auto input_info = session.GetInputTypeInfo(i);

            auto input_tensor_info = input_info.GetTensorTypeAndShapeInfo();

            input_shape = input_tensor_info.GetShape();

            std::cout << "输入名称[" << i << "]: " << input_name << std::endl;

        }

  

        // 6. 输出处理 - 获取输出节点名称

        for(size_t i = 0; i < num_output_nodes; i++) {

            auto output_name_ptr = session.GetOutputNameAllocated(i, allocator);

            std::string output_name = output_name_ptr.get();

            output_names_str.push_back(output_name);

            output_names.push_back(output_names_str.back().c_str());

            std::cout << "输出名称[" << i << "]: " << output_name << std::endl;

        }

  

        std::cout << "[OK] 模型加载成功!" << std::endl;  // 替换Unicode字符

        std::cout << "运行模式: " << (use_gpu ? "GPU" : "CPU") << std::endl;

        std::cout << "输入形状:[";

        for(size_t i = 0; i < input_shape.size(); i++){

            std::cout << input_shape[i];

            if(i != input_shape.size() - 1) std::cout << ",";

        }

        std::cout << "]" << std::endl;

        // 4. 输入准备 - 检查输入形状并给出提示（新功能）

        for(size_t i = 0; i < input_shape.size(); i++){

            if(input_shape[i] < 0){

                std::cout << "注意: 输入形状包含动态维度 [" << i << "] = " << input_shape[i] << std::endl;

                std::cout << "将使用具体形状进行推理" << std::endl;

            }

        }

    }

  

private:

    // 3. 会话创建 - 验证模型文件完整性（新功能）

    void validateModelFile(const std::string& model_path) {

        // 检查文件是否存在

        std::ifstream file(model_path, std::ios::binary);

        if(!file.is_open()){

            throw std::runtime_error("无法打开模型文件: " + model_path);

        }

        // 检查文件大小

        file.seekg(0, std::ios::end);

        size_t file_size = file.tellg();

        file.seekg(0, std::ios::beg);

        std::cout << "模型文件大小: " << file_size << " 字节" << std::endl;

        if(file_size == 0){

            throw std::runtime_error("模型文件为空: " + model_path);

        }

        file.close();

        std::cout << "[OK] 模型文件完整性检查通过" << std::endl;  // 替换Unicode字符

    }

    // 3. 会话创建 - 从内存加载模型（新功能）

    void loadModelFromMemory(const std::string& model_path, Ort::SessionOptions& session_options) {

        std::cout << "从内存加载模型..." << std::endl;

        // 读取整个模型文件到内存

        std::ifstream model_file(model_path, std::ios::binary | std::ios::ate);

        if(!model_file.is_open()){

            throw std::runtime_error("无法打开模型文件进行内存加载: " + model_path);

        }

        size_t file_size = model_file.tellg();

        model_file.seekg(0, std::ios::beg);

        std::vector<char> model_data(file_size);

        if(!model_file.read(model_data.data(), file_size)){

            throw std::runtime_error("读取模型文件到内存失败: " + model_path);

        }

        model_file.close();

        std::cout << "成功读取模型到内存，大小: " << file_size << " 字节" << std::endl;

        try{

            // 3. 会话创建 - 从内存创建会话

            session = Ort::Session(env, model_data.data(), file_size, session_options);

            std::cout << "[OK] 模型内存加载成功" << std::endl;  // 替换Unicode字符

        }catch(const Ort::Exception& e){

            std::string error_msg = "ONNX Runtime错误: " + std::string(e.what());

            std::cout << "错误详情: " << error_msg << std::endl;

            throw std::runtime_error(error_msg);

        }catch(const std::exception& e){

            throw std::runtime_error("模型加载异常: " + std::string(e.what()));

        }

    }

    // 格式化时间显示

    std::string format_duration(long long microseconds) {

        std::stringstream ss;  // 使用stringstream

        if (microseconds < 1000) {

            ss << microseconds << " 微秒";

        } else if (microseconds < 1000000) {

            ss << std::fixed << std::setprecision(2) << (microseconds / 1000.0) << " 毫秒";

        } else {

            ss << std::fixed << std::setprecision(2) << (microseconds / 1000000.0) << " 秒";

        }

        return ss.str();

    }

public:

    // 5. 推理执行 - 预测函数（返回推理时间）

    std::pair<std::vector<float>, long long> predict_with_time(const std::vector<float>& input_data){

        // 4. 输入准备 - 创建内存信息

        auto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);

        // 4. 输入准备 - 使用具体的形状而不是动态形状（新功能）

        std::vector<int64_t> actual_input_shape = {1, 3, 32, 32};

        // 4. 输入准备 - 验证输入数据大小是否匹配（新功能）

        size_t expected_size = 1 * 3 * 32 * 32;

        if(input_data.size() != expected_size){

            throw std::runtime_error("输入数据大小不匹配。期望: " +

                                    std::to_string(expected_size) +

                                    ", 实际: " +

                                    std::to_string(input_data.size()));

        }

        // 4. 输入准备 - 创建输入张量

        Ort::Value input_tensor = Ort::Value::CreateTensor<float>(

            memory_info,

            const_cast<float*>(input_data.data()),

            input_data.size(),

            actual_input_shape.data(),

            actual_input_shape.size()

        );

        // 5. 推理执行 - 运行推理并计时

        auto start_time = std::chrono::high_resolution_clock::now();

        auto output_tensors = session.Run(

            Ort::RunOptions{nullptr},

            input_names.data(),

            &input_tensor,

            1,

            output_names.data(),

            1

        );

  

        auto end_time = std::chrono::high_resolution_clock::now();

        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);

        // 6. 输出处理 - 获取输出数据

        float* output_data = output_tensors[0].GetTensorMutableData<float>();

        size_t output_size = output_tensors[0].GetTensorTypeAndShapeInfo().GetElementCount();

        return std::make_pair(std::vector<float>(output_data, output_data + output_size), duration.count());

    }

    // 6. 输出处理 - 后处理函数，解析推理结果

    int postprocess_results(const std::vector<float>& predictions, int true_label, long long inference_time){

        // 找到概率最高的类别

        int predicted_class = 0;

        float best_score = predictions[0];

        for(size_t i = 1; i < predictions.size(); i++){

            if(predictions[i] > best_score){

                best_score = predictions[i];

                predicted_class = static_cast<int>(i);

            }

        }

        auto class_names = CIFAR10Reader::get_class_names();

        std::cout << "预测类别:" << predicted_class << "(" << class_names[predicted_class] << ")" << std::endl;

        std::cout << "真实类别:" << true_label << "(" << class_names[true_label] << ")" << std::endl;

        std::cout << "置信度:" << best_score << std::endl;

        std::cout << "推理时间:" << format_duration(inference_time) << std::endl;

  

        // 检测预测是否正确

        if(predicted_class == true_label) {

            std::cout << "[OK] 预测正确!" << std::endl;  // 替换Unicode字符

            return 1; // 返回1表示正确

        }

        else{

            std::cout << "[FAIL] 预测错误!" << std::endl;  // 替换Unicode字符

  

            // 显示前3个可能的类别

            std::cout << "其他可能类别:" << std::endl;

            std::vector<std::pair<float, int>> sorted_predictions;

            for(size_t i = 0; i < predictions.size(); ++i){

                sorted_predictions.emplace_back(predictions[i], static_cast<int>(i));

            }

            std::sort(sorted_predictions.rbegin(), sorted_predictions.rend()); // 降序排序

            for(int i = 0; i < 3 && i < static_cast<int>(sorted_predictions.size()); ++i){

                std::cout << "    " << (i + 1) << ". " << class_names[sorted_predictions[i].second]

                          << ": " << sorted_predictions[i].first << std::endl;

            }

            return 0; // 返回0表示错误

        }

    }

    // 使用真实数据进行测试 - 修改为支持所有图片测试

    void test_with_real_data(int num_test_images = -1){

        std::cout << "\n加载真实的CIFAR-10测试数据..." << std::endl;

        auto test_images = CIFAR10Reader::read_test_data();

        if(test_images.empty()){

            std::cerr << "[FAIL] 无法加载测试数据!" << std::endl;  // 替换Unicode字符

            return;

        }

        // 重置统计计数器

        total_tested = 0;

        total_correct = 0;

        total_inference_time = 0;

        // 如果num_test_images为-1，则测试所有图片

        int total_images = static_cast<int>(test_images.size());

        if(num_test_images == -1) {

            num_test_images = total_images;

        }

        int num_test = std::min(num_test_images, total_images);

        std::cout << "[OK] 成功加载测试数据,总图像数量:" << total_images << std::endl;  // 替换Unicode字符

        std::cout << "本次测试图像数量:" << num_test << std::endl;

  

        std::cout << "\n开始推理..." << num_test << "张图像" << std::endl;

        auto class_names = CIFAR10Reader::get_class_names();

        // 添加进度显示

        int progress_interval = std::max(1, num_test / 10); // 每10%显示一次进度

        // 记录总开始时间

        auto total_start_time = std::chrono::high_resolution_clock::now();

        for(int i = 0; i < num_test; ++i){

            const auto& image = test_images[i];

            // 显示进度

            if(i % progress_interval == 0 || i == num_test - 1) {

                std::cout << "进度: " << (i + 1) << "/" << num_test

                          << " (" << ((i + 1) * 100 / num_test) << "%)" << std::endl;

            }

            std::cout << "\n---测试图像" << (i + 1) << "---" << std::endl;

            std::cout << "真实类别:" << image.label << "(" << class_names[image.label] << ")" << std::endl;

  

            try {

                // 5. 推理执行 - 执行推理并获取推理时间

                auto result = predict_with_time(image.data);

                auto predictions = result.first;

                auto inference_time = result.second;

                total_inference_time += inference_time;

                // 6. 输出处理 - 处理结果并统计

                int is_correct = postprocess_results(predictions, image.label, inference_time);

                total_tested++;

                total_correct += is_correct;

            } catch (const std::exception& e) {

                std::cerr << "推理错误: " << e.what() << std::endl;

                continue;

            }

        }

        // 记录总结束时间

        auto total_end_time = std::chrono::high_resolution_clock::now();

        auto total_duration = std::chrono::duration_cast<std::chrono::microseconds>(

            total_end_time - total_start_time);

        // 显示最终统计结果

        display_final_results(total_duration.count());

    }

    // 显示最终测试结果（添加总运行时间参数）

    void display_final_results(long long total_duration_us) {

        std::cout << "\n" << std::string(60, '=') << std::endl;

        std::cout << "最终测试结果统计:" << std::endl;

        std::cout << std::string(60, '=') << std::endl;

        std::cout << "  运行模式: " << (use_gpu ? "GPU" : "CPU") << std::endl;

        std::cout << "  总测试图像数量: " << total_tested << std::endl;

        std::cout << "  正确预测: " << total_correct << std::endl;

        std::cout << "  错误预测: " << (total_tested - total_correct) << std::endl;

        // 时间统计

        std::cout << "\n时间统计:" << std::endl;

        std::cout << "  总运行时间: " << format_duration(total_duration_us) << std::endl;

        std::cout << "  总推理时间: " << format_duration(total_inference_time) << std::endl;

        std::cout << "  数据处理时间: " << format_duration(total_duration_us - total_inference_time) << std::endl;

        if(total_tested > 0) {

            float accuracy = static_cast<float>(total_correct) / total_tested * 100.0f;

            std::cout << "\n准确率统计:" << std::endl;

            std::cout << "  预测准确率: " << std::fixed << std::setprecision(2) << accuracy << "%" << std::endl;

            // 时间效率统计

            double avg_inference_time = static_cast<double>(total_inference_time) / total_tested;

            double avg_total_time = static_cast<double>(total_duration_us) / total_tested;

            double inference_fps = 1000000.0 / avg_inference_time;

            double total_fps = 1000000.0 / avg_total_time;

            std::cout << "\n效率统计:" << std::endl;

            std::cout << "  平均推理时间: " << format_duration(static_cast<long long>(avg_inference_time)) << std::endl;

            std::cout << "  平均总时间: " << format_duration(static_cast<long long>(avg_total_time)) << std::endl;

            std::cout << "  纯推理速度: " << std::fixed << std::setprecision(2) << inference_fps << " FPS" << std::endl;

            std::cout << "  总体速度: " << std::fixed << std::setprecision(2) << total_fps << " FPS" << std::endl;

            // 显示性能评级

            std::cout << "\n性能评级:" << std::endl;

            if(accuracy >= 90.0f) {

                std::cout << "  [优秀] 准确率极高" << std::endl;

            } else if(accuracy >= 80.0f) {

                std::cout << "  [良好] 准确率很好" << std::endl;

            } else if(accuracy >= 70.0f) {

                std::cout << "  [一般] 准确率中等" << std::endl;

            } else if(accuracy >= 60.0f) {

                std::cout << "  [及格] 准确率偏低" << std::endl;

            } else {

                std::cout << "  [需要改进] 准确率不理想" << std::endl;

            }

            // GPU加速效果评估

            if(use_gpu && avg_inference_time < 5000) { // 小于5毫秒认为是好的GPU加速

                std::cout << "  GPU加速效果: [显著]" << std::endl;

            } else if(use_gpu) {

                std::cout << "  GPU加速效果: [一般]" << std::endl;

            } else {

                std::cout << "  运行模式: CPU" << std::endl;

            }

        }

        std::cout << std::string(60, '=') << std::endl;

    }

    // 性能测试函数

    void benchmark_performance(int num_runs = 100){

        std::cout << "\n性能测试 (" << num_runs << "次运行)..." << std::endl;

        // 使用测试数据中的第一张图像

        auto test_images = CIFAR10Reader::read_test_data();

        if(test_images.empty()){

            std::cerr << "无法加载测试数据进行性能测试" << std::endl;

            return;

        }

        const auto& test_data = test_images[0].data;

        // 预热运行

        std::cout << "预热运行..." << std::endl;

        for(int i = 0; i < 10; ++i){

            try {

                auto result = predict_with_time(test_data);

            } catch (const std::exception& e) {

                std::cerr << "预热运行错误: " << e.what() << std::endl;

                return;

            }

        }

        // 正式性能测试

        std::cout << "正式性能测试..." << std::endl;

        auto start_time = std::chrono::high_resolution_clock::now();

        int successful_runs = 0;

        long long total_benchmark_time = 0;

        for(int i = 0; i < num_runs; ++i){

            try {

                // 5. 推理执行 - 执行推理

                auto result = predict_with_time(test_data);

                successful_runs++;

                total_benchmark_time += result.second;

            } catch (const std::exception& e) {

                std::cerr << "性能测试运行 " << i << " 错误: " << e.what() << std::endl;

            }

        }

        auto end_time = std::chrono::high_resolution_clock::now();

        auto total_duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);

        if(successful_runs > 0){

            double avg_time = static_cast<double>(total_benchmark_time) / successful_runs / 1000.0; // 转换为毫秒

            double fps = 1000.0 / avg_time;

            std::cout << "性能统计:" << std::endl;

            std::cout << "  运行模式: " << (use_gpu ? "GPU" : "CPU") << std::endl;

            std::cout << "  总耗时: " << total_duration.count() << "毫秒" << std::endl;

            std::cout << "  成功运行: " << successful_runs << "/" << num_runs << std::endl;

            std::cout << "  平均推理时间: " << avg_time << "毫秒" << std::endl;

            std::cout << "  推理速度: " << fps << "FPS" << std::endl;

            // 显示性能提升对比（新功能）

            if(use_gpu){

                std::cout << "  GPU加速效果: 显著提升" << std::endl;

            }

        } else {

            std::cout << "性能测试失败: 所有运行都出错" << std::endl;

        }

    }

    // 资源清理

    ~CIFAR10Classifier(){

        std::cout << "分类器资源已清理" << std::endl;

    }

};

  

int main(){

    std::cout << "==========================================" << std::endl;

    std::cout << "    CIFAR-10 CNN 图像分类器 (GPU模式)" << std::endl;

    std::cout << "==========================================" << std::endl;

    try{

        // 使用绝对路径确保文件访问

        std::string model_path = "D:/0.Project/0.VS/Py_project/ONNX/CNN_CIFAR-10/models/basic_cnn_cifar10.onnx";

        // 2. 环境初始化 - 创建分类器实例，启用GPU

        CIFAR10Classifier classifier(model_path, true);

        // 测试所有真实数据 - 传入-1表示测试所有图片

        std::cout << "\n开始完整测试集评估..." << std::endl;

        classifier.test_with_real_data(-1);  // -1表示测试所有图片

        // 性能测试

        classifier.benchmark_performance(50);  // 50次运行

        std::cout << "\n完整测试完成！" << std::endl;

    }catch(const std::exception& e){

        std::cerr << "\n错误: " << e.what() << std::endl;

        std::cout << "\n调试建议:" << std::endl;

        std::cout << "1. 检查模型文件路径" << std::endl;

        std::cout << "2. 检查数据文件路径: data/cifar-10-batches-bin/" << std::endl;

        std::cout << "3. 确认ONNX Runtime GPU DLL文件存在" << std::endl;

        std::cout << "4. 检查CUDA和cuDNN安装" << std::endl;

        std::cout << "5. 验证NVIDIA驱动版本" << std::endl;

        return -1;

    }

    return 0;

}
```
