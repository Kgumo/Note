数据预处理(preprocessing.ipynb)
```python
import os
import cv2
import pandas as pd
import numpy as np
import yaml
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from sklearn.model_selection import train_test_split
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
class DataAnalyzer:

    def __init__(self, data_root='data'):

        self.data_root = data_root

        self.images_dir = os.path.join(data_root, 'images')

        self.labels_dir = os.path.join(data_root, 'labels')

        classes_path = os.path.join(data_root, 'classes.txt')

        with open(classes_path, 'r', encoding='utf-8') as f:

            self.class_names = [line.strip() for line in f.readlines()]

        self.num_classes = len(self.class_names)

    def analyze_dataset(self):

        train_images = os.listdir(os.path.join(self.images_dir, 'train'))

        train_labels = os.listdir(os.path.join(self.labels_dir, 'train'))

        test_images = os.listdir(os.path.join(self.images_dir, 'test'))

        test_labels = os.listdir(os.path.join(self.labels_dir, 'test'))

        val_images = os.listdir(os.path.join(self.images_dir, 'val'))  

        val_labels = os.listdir(os.path.join(self.labels_dir, 'val'))

        class_counts = {i: 0 for i in range(self.num_classes)}

        image_with_damage = 0

        total_bboxes = 0

        bbox_sizes = []

        for label_file in train_labels:

            label_path = os.path.join(self.labels_dir, 'train', label_file)

            with open(label_path, 'r') as f:

                lines = f.readlines()

                if len(lines) > 0:

                    image_with_damage += 1

                    total_bboxes += len(lines)

                    for line in lines:

                        class_id = int(line.strip().split()[0])

                        class_counts[class_id] += 1

                        _, x_center, y_center, width, height = map(float, line.strip().split())

                        bbox_sizes.append(width * height)

        return class_counts, image_with_damage, total_bboxes, len(train_images), len(test_images), len(val_images)

    def check_image_sizes(self, sample_size=50):

        train_images_dir = os.path.join(self.images_dir, 'train')

        image_files = [f for f in os.listdir(train_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        if len(image_files) > sample_size:

            sample_files = np.random.choice(image_files, sample_size, replace=False)

        else:

            sample_files = image_files

        sizes = []

        for img_file in sample_files:

            img_path = os.path.join(train_images_dir, img_file)

            with Image.open(img_path) as img:

                sizes.append(img.size)

        return np.array(sizes)

    def visualize_samples(self, num_samples=5, save_dir='output/visualization'):

        os.makedirs(save_dir, exist_ok=True)

        train_images_dir = os.path.join(self.images_dir, 'train')

        train_labels_dir = os.path.join(self.labels_dir, 'train')

        image_files = [f for f in os.listdir(train_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        selected_images = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)

        for i, img_file in enumerate(selected_images):

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            img_path = os.path.join(train_images_dir, img_file)

            image = Image.open(img_path)

            img_width, img_height = image.size

            ax1.imshow(image)

            ax1.set_title(f'{img_file}\n({img_width}x{img_height})')

            ax1.axis('off')

            ax2.imshow(image)

            label_file = os.path.splitext(img_file)[0] + '.txt'

            label_path = os.path.join(train_labels_dir, label_file)

            bbox_count = 0

            if os.path.exists(label_path):

                with open(label_path, 'r') as f:

                    for line in f.readlines():

                        data = line.strip().split()

                        if len(data) == 5:

                            class_id = int(data[0])

                            x_center, y_center, width, height = map(float, data[1:])

                            x_center_px = x_center * img_width

                            y_center_px = y_center * img_height

                            width_px = width * img_width

                            height_px = height * img_height

                            x_min = x_center_px - width_px / 2

                            y_min = y_center_px - height_px / 2

                            rect = patches.Rectangle(

                                (x_min, y_min), width_px, height_px,

                                linewidth=2, edgecolor='red', facecolor='none'

                            )

                            ax2.add_patch(rect)

                            ax2.text(

                                x_min, y_min - 5, f'{self.class_names[class_id]}',

                                bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.7),

                                fontsize=10, color='white', weight='bold'

                            )

                            bbox_count += 1

            ax2.set_title(f'标注图像 ({bbox_count}个目标)')

            ax2.axis('off')

            plt.tight_layout()

            save_path = os.path.join(save_dir, f'sample_{i+1}.png')

            plt.savefig(save_path, dpi=150, bbox_inches='tight')

            plt.close()

    def create_class_distribution_plot(self, class_counts, save_path='output/class_distribution.png'):

        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        classes = [self.class_names[i] for i in range(len(self.class_names))]

        counts = [class_counts[i] for i in range(len(self.class_names))]

        plt.figure(figsize=(10, 6))

        bars = plt.bar(classes, counts, color=['#ff9999', '#66b3ff', '#99ff99'])

        plt.title('类别分布', fontsize=16, fontweight='bold')

        plt.xlabel('类别', fontsize=12)

        plt.ylabel('样本数量', fontsize=12)

        for bar, count in zip(bars, counts):

            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,

                    str(count), ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()

        plt.savefig(save_path, dpi=150, bbox_inches='tight')

        plt.show()
        os.makedirs('output', exist_ok=True)

  

analyzer = DataAnalyzer('data')

  

class_counts, damaged_images, total_bboxes, train_count, test_count, val_count = analyzer.analyze_dataset()

  

print(f"训练集: {train_count} 张图片")

print(f"验证集: {val_count} 张图片")

print(f"测试集: {test_count} 张图片")

print(f"有破损图片: {damaged_images}")

print(f"总边界框: {total_bboxes}")

print(f"各类别数量: {dict(class_counts)}")

sizes = analyzer.check_image_sizes(sample_size=100)

  

print(f"图像尺寸范围: {sizes[:, 0].min()}x{sizes[:, 1].min()} - {sizes[:, 0].max()}x{sizes[:, 1].max()}")

print(f"平均尺寸: {sizes.mean(axis=0).astype(int)}")

analyzer.visualize_samples(num_samples=6)

report = {

    'total_classes': analyzer.num_classes,

    'class_names': analyzer.class_names,

    'class_distribution': class_counts,

    'training_images': train_count,

    'validation_images': val_count,

    'test_images': test_count,

    'damaged_images': damaged_images,

    'total_bboxes': total_bboxes,

    'avg_image_size': f"{sizes.mean(axis=0).astype(int)}"

}

  

with open('output/data_analysis_report.yaml', 'w') as f:

    yaml.dump(report, f, default_flow_style=False)
```
模型训练文件(train_model.ipynb)
```python
import torch

import os

import yaml

from ultralytics import YOLO

import matplotlib.pyplot as plt

import pandas as pd

import numpy as np

import glob

from PIL import Image

  

print(f"PyTorch版本: {torch.__version__}")

if torch.cuda.is_available():

    print(f"GPU设备: {torch.cuda.get_device_name(0)}")

torch.cuda.empty_cache()

with open('dataset.yaml', 'r') as f:

    dataset_config = yaml.safe_load(f)

  

print("数据集配置:")

print("=" * 50)

for key, value in dataset_config.items():

    print(f"{key}: {value}")

print("=" * 50)

model_name = 'yolov8s.pt'

device = 'cuda' if torch.cuda.is_available() else 'cpu'

  

training_config = {

    'data': 'dataset.yaml',

    'epochs': 250,

    'imgsz': 640,

    'batch': 16,

    'patience': 100,

    'save': True,

    'device': device,

    'workers': 8,

    'optimizer': 'auto',

    'lr0': 0.01,

    'lrf': 0.1,

    'weight_decay': 0.0005,

    'warmup_epochs': 3.0,

    'box': 7.5,

    'cls': 0.8,

    'dfl': 1.5,

    'close_mosaic': 10,

    'name': 'container_detection_amp',

    'exist_ok': True,

    'amp': True,

    'half': False,

    'plots': True,

    'save_period': 10,

    'val': True,

    'cache': False,

}

  

print(f"Device: {device}")

print(f"AMP: {training_config['amp']}")

if torch.cuda.is_available():

    torch.cuda.empty_cache()

  

model = YOLO(model_name)

results = model.train(**training_config)

def plot_training_results(results_dir):

    results_csv = os.path.join(results_dir, 'results.csv')

    if not os.path.exists(results_csv):

        print(f"未找到结果文件: {results_csv}")

        return None

    results_df = pd.read_csv(results_csv)

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    if 'train/box_loss' in results_df.columns:

        ax1.plot(results_df['train/box_loss'], label='Box Loss', color='blue', linewidth=2)

    if 'train/cls_loss' in results_df.columns:

        ax1.plot(results_df['train/cls_loss'], label='Cls Loss', color='red', linewidth=2)

    if 'train/dfl_loss' in results_df.columns:

        ax1.plot(results_df['train/dfl_loss'], label='DFL Loss', color='green', linewidth=2)

    ax1.set_title('Training Loss')

    ax1.set_xlabel('Epoch')

    ax1.set_ylabel('Loss')

    ax1.legend()

    ax1.grid(True, alpha=0.3)

    if 'val/box_loss' in results_df.columns:

        ax2.plot(results_df['val/box_loss'], label='Box Loss', color='blue', linewidth=2)

    if 'val/cls_loss' in results_df.columns:

        ax2.plot(results_df['val/cls_loss'], label='Cls Loss', color='red', linewidth=2)

    if 'val/dfl_loss' in results_df.columns:

        ax2.plot(results_df['val/dfl_loss'], label='DFL Loss', color='green', linewidth=2)

    ax2.set_title('Validation Loss')

    ax2.set_xlabel('Epoch')

    ax2.set_ylabel('Loss')

    ax2.legend()

    ax2.grid(True, alpha=0.3)

    if 'metrics/precision(B)' in results_df.columns:

        ax3.plot(results_df['metrics/precision(B)'], label='Precision', color='purple', linewidth=2)

    if 'metrics/recall(B)' in results_df.columns:

        ax3.plot(results_df['metrics/recall(B)'], label='Recall', color='orange', linewidth=2)

    ax3.set_title('Precision & Recall')

    ax3.set_xlabel('Epoch')

    ax3.set_ylabel('Score')

    ax3.legend()

    ax3.grid(True, alpha=0.3)

    if 'metrics/mAP50(B)' in results_df.columns:

        ax4.plot(results_df['metrics/mAP50(B)'], label='mAP50', color='red', linewidth=2)

    if 'metrics/mAP50-95(B)' in results_df.columns:

        ax4.plot(results_df['metrics/mAP50-95(B)'], label='mAP50-95', color='blue', linewidth=2)

    ax4.set_title('mAP Metrics')

    ax4.set_xlabel('Epoch')

    ax4.set_ylabel('mAP')

    ax4.legend()

    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    plt.show()

    return results_df

  

if latest_train:

    results_df = plot_training_results(latest_train)

    if results_df is not None and not results_df.empty:

        last_row = results_df.iloc[-1]

        print("最终训练指标:")

        metrics_to_show = {

            'metrics/precision(B)': '精确率',

            'metrics/recall(B)': '召回率',

            'metrics/mAP50(B)': 'mAP50',

            'metrics/mAP50-95(B)': 'mAP50-95'

        }

        for col, name in metrics_to_show.items():

            if col in last_row and not pd.isna(last_row[col]):

                print(f"{name}: {last_row[col]:.4f}")

def validate_model(model_path):

    print("验证模型...")

    model = YOLO(model_path)

    metrics = model.val(

        data='dataset.yaml',

        split='val',

        imgsz=640,

        batch=8,

        save_json=True,

        conf=0.25,

        iou=0.6

    )

    print("验证结果:")

    print(f"mAP50: {metrics.box.map50:.4f}")

    print(f"mAP50-95: {metrics.box.map:.4f}")

    class_names = {0: '凹陷', 1: '破洞', 2: '锈蚀'}

    for i in range(len(metrics.box.ap50)):

        class_name = class_names.get(i, f'Class_{i}')

        ap50 = metrics.box.ap50[i]

        ap = metrics.box.ap[i] if i < len(metrics.box.ap) else 0

        print(f"{class_name}: AP50={ap50:.4f}, AP={ap:.4f}")

    return metrics

  

if latest_train and os.path.exists(os.path.join(latest_train, 'weights', 'best.pt')):

    best_model_path = os.path.join(latest_train, 'weights', 'best.pt')

    metrics = validate_model(best_model_path)
    
    def save_training_summary(results_dir, metrics, config):

    results_csv = os.path.join(results_dir, 'results.csv')

    final_metrics = {}

    if os.path.exists(results_csv):

        results_df = pd.read_csv(results_csv)

        if not results_df.empty:

            last_row = results_df.iloc[-1]

            final_metrics = {

                'final_precision': float(last_row.get('metrics/precision(B)', 0)),

                'final_recall': float(last_row.get('metrics/recall(B)', 0)),

                'final_map50': float(last_row.get('metrics/mAP50(B)', 0)),

                'final_map50_95': float(last_row.get('metrics/mAP50-95(B)', 0))

            }

    summary = {

        'training_config': config,

        'validation_metrics': {

            'mAP50': float(metrics.box.map50) if metrics else 0,

            'mAP50_95': float(metrics.box.map) if metrics else 0,

        },

        'final_training_metrics': final_metrics,

        'model_info': {

            'model_name': config.get('name', 'unknown'),

            'best_model_path': os.path.join(results_dir, 'weights', 'best.pt') if results_dir else 'unknown',

            'training_completed': True,

            'amp_enabled': config.get('amp', False)

        }

    }

    summary_path = os.path.join(results_dir, 'training_summary.yaml') if results_dir else 'training_summary.yaml'

    with open(summary_path, 'w') as f:

        yaml.dump(summary, f, default_flow_style=False)

    print(f"训练总结已保存: {summary_path}")

    return summary

  

if latest_train:

    summary = save_training_summary(latest_train, metrics, training_config)
   
 print("训练完成!")

if latest_train:

    best_model_path = os.path.join(latest_train, 'weights', 'best.pt')

    print(f"最佳模型路径: {best_model_path}")
    ```
 
 ### 预测与检验(predict_and_visualize.ipynb)
 
```python
import torch

from ultralytics import YOLO

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import matplotlib.patches as patches

from PIL import Image

import os

import glob

import random

import time

import yaml

  

plt.rcParams['font.sans-serif'] = ['SimHei']

plt.rcParams['axes.unicode_minus'] = False

  

print(f"PyTorch版本: {torch.__version__}")

print(f"CUDA可用: {torch.cuda.is_available()}")

def find_best_model():

    train_dirs = glob.glob('runs/detect/*')

    if not train_dirs:

        train_dirs = glob.glob('./runs/detect/*')

        if not train_dirs:

            print("未找到训练结果目录")

            return None, None, None

    train_dirs.sort(key=os.path.getmtime, reverse=True)

    for latest_train in train_dirs:

        weights_dir = os.path.join(latest_train, 'weights')

        if not os.path.exists(weights_dir):

            continue

        best_model_path = os.path.join(weights_dir, 'best.pt')

        if os.path.exists(best_model_path):

            print(f"找到模型: {best_model_path}")

            map50 = None

            map50_95 = None

            summary_path = os.path.join(latest_train, 'training_summary.yaml')

            if os.path.exists(summary_path):

                try:

                    with open(summary_path, 'r', encoding='utf-8') as f:

                        summary = yaml.safe_load(f)

                        if summary and 'validation_metrics' in summary:

                            map50 = summary['validation_metrics'].get('mAP50', None)

                            map50_95 = summary['validation_metrics'].get('mAP50_95', None)

                except Exception as e:

                    print(f"读取评估指标失败: {e}")

            return best_model_path, (map50, map50_95), latest_train

    print("未找到有效的模型文件")

    return None, None, None

  

best_model_path, metrics, train_dir = find_best_model()

if best_model_path:

    from ultralytics import YOLO

    model = YOLO(best_model_path)

    map50, map50_95 = metrics if metrics else (None, None)

    print(f"模型加载成功! mAP50: {map50}, mAP50-95: {map50_95}")

else:

    print("无法加载模型")
    
def predict_test_set(model, conf_threshold=0.25, batch_size=8):

    print("开始预测...")

    test_images_dir = 'data/images/test'

    test_images = glob.glob(os.path.join(test_images_dir, '*.jpg')) + \

                  glob.glob(os.path.join(test_images_dir, '*.jpeg')) + \

                  glob.glob(os.path.join(test_images_dir, '*.png'))

    print(f"测试图片数量: {len(test_images)}")

    if len(test_images) == 0:

        print("未找到测试图片")

        return []

    predictions = []

    start_time = time.time()

    for i in range(0, len(test_images), batch_size):

        batch_images = test_images[i:i+batch_size]

        if i % 50 == 0:

            print(f"处理进度: {i+1}/{len(test_images)}")

        batch_results = model.predict(batch_images, save=False, verbose=False,

                                    conf=conf_threshold, imgsz=640)

        for j, result in enumerate(batch_results):

            img_path = batch_images[j]

            img_filename = os.path.basename(img_path)

            boxes = result.boxes

            if boxes is not None:

                for box in boxes:

                    class_id = int(box.cls.item())

                    bbox = box.xywhn[0].cpu().numpy()

                    conf = box.conf.item()

                    predictions.append({

                        'image_id': img_filename,

                        'class_id': class_id,

                        'x_center': bbox[0],

                        'y_center': bbox[1],

                        'width': bbox[2],

                        'height': bbox[3],

                        'confidence': conf

                    })

    end_time = time.time()

    print(f"预测完成，耗时: {end_time - start_time:.2f}秒")

    print(f"检测目标总数: {len(predictions)}")

    return predictions

  

if best_model_path:

    predictions = predict_test_set(model, batch_size=8)

else:

    predictions = []
def save_predictions(predictions, output_file='test_result.csv'):

    if not predictions:

        print("没有预测结果")

        return None

    df = pd.DataFrame(predictions)

    df.to_csv(output_file, index=False)

    print(f"结果已保存: {output_file}")

    class_counts = df['class_id'].value_counts().sort_index()

    print("类别分布:")

    class_names = {0: '凹陷', 1: '破洞', 2: '锈蚀'}

    for class_id, count in class_counts.items():

        class_name = class_names.get(class_id, f'Class_{class_id}')

        percentage = count / len(predictions) * 100

        print(f"  {class_name}: {count}个 ({percentage:.1f}%)")

    images_count = df['image_id'].nunique()

    avg_objects_per_image = len(predictions) / images_count if images_count > 0 else 0

    print(f"平均每张图片目标数: {avg_objects_per_image:.2f}")

    return df

  

if predictions:

    result_df = save_predictions(predictions)

else:

    result_df = None
    
def visualize_predictions(model, map50=None, map50_95=None, num_samples=6):

    test_images_dir = 'data/images/test'

    test_images = glob.glob(os.path.join(test_images_dir, '*.jpg')) + \

                  glob.glob(os.path.join(test_images_dir, '*.jpeg')) + \

                  glob.glob(os.path.join(test_images_dir, '*.png'))

    if len(test_images) == 0:

        print("未找到测试图片")

        return

    if len(test_images) > num_samples:

        sample_images = random.sample(test_images, num_samples)

    else:

        sample_images = test_images

    cols = 3

    rows = (num_samples + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))

    if num_samples == 1:

        axes = np.array([axes]).reshape(1, 1)

    elif rows == 1:

        axes = axes.reshape(1, -1)

    class_names = {0: '凹陷', 1: '破洞', 2: '锈蚀'}

    colors = {0: 'red', 1: 'blue', 2: 'green'}

    batch_results = model.predict(sample_images, save=False, verbose=False, conf=0.25)

    for idx, (img_path, results) in enumerate(zip(sample_images, batch_results)):

        if idx >= num_samples:

            break

        row = idx // cols

        col = idx % cols

        image = Image.open(img_path)

        img_width, img_height = image.size

        ax = axes[row, col] if rows > 1 else axes[col]

        ax.imshow(image)

        ax.set_title(f'{os.path.basename(img_path)}', fontsize=10)

        detection_count = 0

        boxes = results.boxes

        if boxes is not None:

            for box in boxes:

                class_id = int(box.cls.item())

                bbox = box.xywhn[0].cpu().numpy()

                conf = box.conf.item()

                x_center = bbox[0] * img_width

                y_center = bbox[1] * img_height

                width = bbox[2] * img_width

                height = bbox[3] * img_height

                x_min = x_center - width / 2

                y_min = y_center - height / 2

                rect = patches.Rectangle(

                    (x_min, y_min), width, height,

                    linewidth=2,

                    edgecolor=colors.get(class_id, 'red'),

                    facecolor='none'

                )

                ax.add_patch(rect)

                label = f'{class_names.get(class_id, class_id)} {conf:.2f}'

                ax.text(

                    x_min, y_min - 5, label,

                    bbox=dict(boxstyle="round,pad=0.3", facecolor=colors.get(class_id, 'red'), alpha=0.7),

                    fontsize=8, color='white', fontweight='bold'

                )

                detection_count += 1

        ax.text(0.95, 0.95, f'检测: {detection_count}',

                transform=ax.transAxes, ha='right', va='top',

                bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8),

                fontsize=9, fontweight='bold')

        ax.axis('off')

    for idx in range(len(sample_images), rows * cols):

        row = idx // cols

        col = idx % cols

        ax = axes[row, col] if rows > 1 else axes[col]

        ax.axis('off')

    if map50 is not None and map50_95 is not None:

        fig.suptitle(f'集装箱破损检测 - mAP50={map50:.3f}, mAP50-95={map50_95:.3f}',

                    fontsize=16, fontweight='bold', y=0.98)

    else:

        fig.suptitle('集装箱破损检测', fontsize=16, fontweight='bold', y=0.98)

    plt.tight_layout()

    plt.subplots_adjust(top=0.93)  

    plt.show()

    print(f"可视化完成: {len(sample_images)} 张图片")

  

if best_model_path:

    visualize_predictions(model, map50, map50_95, num_samples=6)
    
def performance_analysis(predictions_df, map50=None, map50_95=None):

    if predictions_df is None or predictions_df.empty:

        print("没有预测数据")

        return

    print("性能分析:")

    if map50 is not None and map50_95 is not None:

        print(f"mAP50: {map50:.4f}")

        print(f"mAP50-95: {map50_95:.4f}")

    total_images = predictions_df['image_id'].nunique()

    total_detections = len(predictions_df)

    print(f"总图片数: {total_images}")

    print(f"总检测数: {total_detections}")

    print(f"平均每张图片检测数: {total_detections/total_images:.2f}")

    class_names = {0: '凹陷', 1: '破洞', 2: '锈蚀'}

    class_distribution = predictions_df['class_id'].value_counts().sort_index()

    print("检测类别分布:")

    for class_id, count in class_distribution.items():

        class_name = class_names.get(class_id, f'Class_{class_id}')

        percentage = count / total_detections * 100

        print(f"  {class_name}: {count}次 ({percentage:.1f}%)")

    detections_per_image = predictions_df.groupby('image_id').size()

    print(f"最多检测数: {detections_per_image.max()}")

    print(f"最少检测数: {detections_per_image.min()}")

    print(f"平均检测数: {detections_per_image.mean():.2f}")

  

if result_df is not None:

    performance_analysis(result_df, map50, map50_95)
```