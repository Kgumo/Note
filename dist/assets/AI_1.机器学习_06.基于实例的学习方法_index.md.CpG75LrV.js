import{_ as e,c as r,o as i,a3 as n}from"./chunks/framework.CkaDlzKP.js";const K=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"AI/1.机器学习/06.基于实例的学习方法/index.md","filePath":"AI/1.机器学习/06.基于实例的学习方法/index.md","lastUpdated":1754900608000}'),t={name:"AI/1.机器学习/06.基于实例的学习方法/index.md"};function a(o,l,s,d,g,u){return i(),r("div",null,l[0]||(l[0]=[n('<h3 id="基于实例的学习-instance-based-learning-ibl" tabindex="-1"><strong>基于实例的学习（Instance-Based Learning, IBL）</strong> <a class="header-anchor" href="#基于实例的学习-instance-based-learning-ibl" aria-label="Permalink to &quot;**基于实例的学习（Instance-Based Learning, IBL）**&quot;">​</a></h3><p>基于实例的学习是一种非参数化学习方法，它不显式地学习一个模型，而是直接利用训练数据中的实例进行预测。其核心思想是“相似的输入会产生相似的输出”。K近邻算法（KNN）是IBL的典型代表。</p><hr><h3 id="k近邻算法-knn" tabindex="-1"><strong>K近邻算法（KNN）</strong> <a class="header-anchor" href="#k近邻算法-knn" aria-label="Permalink to &quot;**K近邻算法（KNN）**&quot;">​</a></h3><p>KNN是一种简单但强大的分类和回归算法。其基本思想是：对于一个新的输入实例，找到训练集中与其最接近的K个邻居，然后根据这些邻居的标签或值来预测新实例的标签或值。</p><h4 id="knn的基本算法" tabindex="-1"><strong>KNN的基本算法</strong> <a class="header-anchor" href="#knn的基本算法" aria-label="Permalink to &quot;**KNN的基本算法**&quot;">​</a></h4><ol><li><strong>输入</strong>： <ul><li>训练数据集 ( D = {(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)} )，其中 ( x_i ) 是特征向量，( y_i ) 是标签或值。</li><li>新实例 ( x_{\\text{new}} )。</li><li>参数K（邻居的数量）。</li></ul></li><li><strong>步骤</strong>： <ul><li>计算 ( x_{\\text{new}} ) 与训练集中每个实例 ( x_i ) 的距离。</li><li>选择距离最近的K个邻居。</li><li>对于分类任务，采用多数投票法确定 ( x_{\\text{new}} ) 的标签；对于回归任务，采用K个邻居的平均值作为预测值。</li></ul></li><li><strong>输出</strong>： <ul><li>预测的标签或值。</li></ul></li></ol><h4 id="knn的讨论" tabindex="-1"><strong>KNN的讨论</strong> <a class="header-anchor" href="#knn的讨论" aria-label="Permalink to &quot;**KNN的讨论**&quot;">​</a></h4><ol><li><strong>距离度量</strong>： <ul><li>常用的距离度量包括欧氏距离、曼哈顿距离、余弦相似度等。</li><li>欧氏距离：( d(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} )。</li></ul></li><li><strong>属性处理</strong>： <ul><li><strong>归一化</strong>：不同特征的取值范围可能差异很大，需要对特征进行归一化，以避免某些特征主导距离计算。</li><li><strong>加权</strong>：可以为不同特征赋予不同的权重，以反映其重要性。</li></ul></li><li><strong>连续取值目标函数</strong>： <ul><li>对于回归任务，KNN可以直接预测连续值，通常采用K个邻居的平均值。</li></ul></li><li><strong>K的选择</strong>： <ul><li>K的选择对模型性能有很大影响。K太小容易过拟合，K太大容易欠拟合。</li><li>通常通过交叉验证来选择最优的K值。</li></ul></li><li><strong>打破平局</strong>： <ul><li>当K个邻居的投票结果出现平局时，可以采用加权投票或随机选择的方式打破平局。</li></ul></li></ol><h4 id="knn的效率问题" tabindex="-1"><strong>KNN的效率问题</strong> <a class="header-anchor" href="#knn的效率问题" aria-label="Permalink to &quot;**KNN的效率问题**&quot;">​</a></h4><p>KNN的主要缺点是计算开销大，尤其是在高维数据和大规模数据集上。为了提高效率，可以采用以下方法：</p><ol><li><strong>KD-Tree</strong>：一种基于树的数据结构，用于加速最近邻搜索。</li><li><strong>近似最近邻搜索</strong>：通过牺牲一定的精度来换取计算效率。</li></ol><hr><h3 id="kd-tree-k-dimensional-tree" tabindex="-1"><strong>KD-Tree（K-Dimensional Tree）</strong> <a class="header-anchor" href="#kd-tree-k-dimensional-tree" aria-label="Permalink to &quot;**KD-Tree（K-Dimensional Tree）**&quot;">​</a></h3><p>KD-Tree是一种用于高效搜索K维空间中最近邻的数据结构。它通过递归地将数据空间划分为更小的区域，从而减少搜索范围。</p><h4 id="kd-tree的构建" tabindex="-1"><strong>KD-Tree的构建</strong> <a class="header-anchor" href="#kd-tree的构建" aria-label="Permalink to &quot;**KD-Tree的构建**&quot;">​</a></h4><ol><li><strong>分割维度的选择</strong>： <ul><li>通常选择范围最宽的维度进行分割。</li></ul></li><li><strong>分割值的选择</strong>： <ul><li>选择数据点在分割维度上的中位数作为分割值。选择中位数可以保证树的平衡。</li></ul></li><li><strong>停止条件</strong>： <ul><li>当剩余的数据点少于预设阈值，或者区域的宽度达到最小值时，停止分割。</li></ul></li></ol><h4 id="kd-tree的查询" tabindex="-1"><strong>KD-Tree的查询</strong> <a class="header-anchor" href="#kd-tree的查询" aria-label="Permalink to &quot;**KD-Tree的查询**&quot;">​</a></h4><ol><li><strong>遍历树</strong>： <ul><li>从根节点开始，递归地向下遍历树，找到包含查询点的叶节点。</li></ul></li><li><strong>回溯搜索</strong>： <ul><li>在回溯过程中，检查其他分支是否可能包含更近的邻居。如果可能，则继续搜索。</li></ul></li></ol><h4 id="kd-tree的优点" tabindex="-1"><strong>KD-Tree的优点</strong> <a class="header-anchor" href="#kd-tree的优点" aria-label="Permalink to &quot;**KD-Tree的优点**&quot;">​</a></h4><ul><li>适用于低维数据（维度通常不超过20）。</li><li>可以显著减少最近邻搜索的时间复杂度。</li></ul><h4 id="kd-tree的局限性" tabindex="-1"><strong>KD-Tree的局限性</strong> <a class="header-anchor" href="#kd-tree的局限性" aria-label="Permalink to &quot;**KD-Tree的局限性**&quot;">​</a></h4><ul><li>在高维数据中，KD-Tree的效率会显著下降（“维度灾难”问题）。</li><li>构建和查询KD-Tree的开销可能较大。</li></ul><hr><h3 id="基于实例的学习与其他方法的关系" tabindex="-1"><strong>基于实例的学习与其他方法的关系</strong> <a class="header-anchor" href="#基于实例的学习与其他方法的关系" aria-label="Permalink to &quot;**基于实例的学习与其他方法的关系**&quot;">​</a></h3><ol><li><strong>贝叶斯学习</strong>： <ul><li>贝叶斯学习是一种基于概率模型的参数化方法，而IBL是非参数化方法。</li></ul></li><li><strong>支持向量机（SVM）</strong>： <ul><li>SVM通过寻找最大间隔超平面进行分类，而IBL直接利用训练数据中的实例进行预测。</li></ul></li><li><strong>无监督学习</strong>： <ul><li>IBL通常用于监督学习任务，但也可以扩展到无监督学习，如K均值聚类。</li></ul></li></ol><hr><h3 id="总结" tabindex="-1"><strong>总结</strong> <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;**总结**&quot;">​</a></h3><p>基于实例的学习（特别是KNN）是一种简单但有效的机器学习方法，适用于小规模、低维数据。其主要特点包括：</p><ul><li>无需显式训练模型，直接利用训练数据进行预测。</li><li>对数据分布没有假设，适用于非线性问题。</li><li>计算开销较大，但可以通过KD-Tree等数据结构进行优化。</li></ul><p>在实际应用中，需要根据具体任务和数据特点选择合适的距离度量、K值以及优化方法（如KD-Tree）。</p>',31)]))}const c=e(t,[["render",a]]);export{K as __pageData,c as default};
