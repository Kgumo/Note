import{_ as e,c as m,o as l,a3 as t,j as s,a}from"./chunks/framework.CkaDlzKP.js";const g=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"AI/1.机器学习/05.贝叶斯学习/index.md","filePath":"AI/1.机器学习/05.贝叶斯学习/index.md","lastUpdated":1754900608000}'),p={name:"AI/1.机器学习/05.贝叶斯学习/index.md"};function i(r,n,o,c,h,d){return l(),m("div",null,n[0]||(n[0]=[t('<h3 id="一、贝叶斯定理的核心思想" tabindex="-1"><strong>一、贝叶斯定理的核心思想</strong> <a class="header-anchor" href="#一、贝叶斯定理的核心思想" aria-label="Permalink to &quot;**一、贝叶斯定理的核心思想**&quot;">​</a></h3><ol><li><strong>公式与术语</strong></li></ol><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>h</mi><mi mathvariant="normal">∣</mi><mi>D</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>h</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>h</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex"> P(h|D) = \\frac{P(D|h)P(h)}{P(D)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em;"></span><span class="strut bottom" style="height:2.363em;vertical-align:-0.936em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">h</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span><span style="top:-0.2300000000000001em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mord mathrm">∣</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">h</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li><strong>后验概率</strong> ( P(h|D) ): 观察到数据 ( D ) 后假设 ( h ) 的修正概率</li><li><strong>先验概率</strong> ( P(h) ): 未观察数据前的初始信念（如人群癌症患病率0.008）</li><li><strong>似然度</strong> ( P(D|h) ): 假设 ( h ) 下数据 ( D ) 的生成概率（如癌症患者检测阳性概率98%）</li><li><strong>证据因子</strong> ( P(D) ): 数据的边际概率（通常通过全概率公式计算）</li></ul>',4),s("ol",{start:"2"},[s("li",null,[s("p",null,[s("strong",null,"医学诊断示例")]),s("ul",null,[s("li",null,[a("已知："),s("br"),a(" -$$ ( P(\\text{cancer}) = 0.008 ), ( P(+|\\text{cancer}) = 0.98 ), ( P(+|\\neg\\text{cancer}) = 0.03 ) $$")]),s("li",null,"计算后验概率：")]),s("p",null,[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",null,[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",null,"("),s("mtext",null,[s("mi",{mathvariant:"normal"},"c"),s("mi",{mathvariant:"normal"},"a"),s("mi",{mathvariant:"normal"},"n"),s("mi",{mathvariant:"normal"},"c"),s("mi",{mathvariant:"normal"},"e"),s("mi",{mathvariant:"normal"},"r")]),s("mi",{mathvariant:"normal"},"∣"),s("mo",null,"+"),s("mo",null,")"),s("mo",null,"="),s("mfrac",null,[s("mrow",null,[s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"9"),s("mn",null,"8"),s("mo",null,"×"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"0"),s("mn",null,"0"),s("mn",null,"8")]),s("mrow",null,[s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"9"),s("mn",null,"8"),s("mo",null,"×"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"0"),s("mn",null,"0"),s("mn",null,"8"),s("mo",null,"+"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"0"),s("mn",null,"3"),s("mo",null,"×"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"9"),s("mn",null,"9"),s("mn",null,"2")])]),s("mo",null,"≈"),s("mn",null,"0"),s("mi",{mathvariant:"normal"},"."),s("mn",null,"2"),s("mn",null,"0"),s("mn",null,"8")]),s("annotation",{encoding:"application/x-tex"},"P(\\text{cancer}|+) = \\frac{0.98 \\times 0.008}{0.98 \\times 0.008 + 0.03 \\times 0.992} \\approx 0.208")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"strut",style:{height:"1.32144em"}}),s("span",{class:"strut bottom",style:{height:"2.09077em","vertical-align":"-0.7693300000000001em"}}),s("span",{class:"base displaystyle textstyle uncramped"},[s("span",{class:"mord mathit",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"text mord displaystyle textstyle uncramped"},[s("span",{class:"mord mathrm"},"c"),s("span",{class:"mord mathrm"},"a"),s("span",{class:"mord mathrm"},"n"),s("span",{class:"mord mathrm"},"c"),s("span",{class:"mord mathrm"},"e"),s("span",{class:"mord mathrm"},"r")]),s("span",{class:"mord mathrm"},"∣"),s("span",{class:"mbin"},"+"),s("span",{class:"mclose"},")"),s("span",{class:"mrel"},"="),s("span",{class:"mord reset-textstyle displaystyle textstyle uncramped"},[s("span",{class:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist"},[s("span",{style:{top:"0.686em"}},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),s("span",{class:"reset-textstyle textstyle cramped"},[s("span",{class:"mord textstyle cramped"},[s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"9"),s("span",{class:"mord mathrm"},"8"),s("span",{class:"mbin"},"×"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"8"),s("span",{class:"mbin"},"+"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"3"),s("span",{class:"mbin"},"×"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"9"),s("span",{class:"mord mathrm"},"9"),s("span",{class:"mord mathrm"},"2")])])]),s("span",{style:{top:"-0.22999999999999998em"}},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),s("span",{class:"reset-textstyle textstyle uncramped frac-line"})]),s("span",{style:{top:"-0.677em"}},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),s("span",{class:"reset-textstyle textstyle uncramped"},[s("span",{class:"mord textstyle uncramped"},[s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"9"),s("span",{class:"mord mathrm"},"8"),s("span",{class:"mbin"},"×"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"8")])])]),s("span",{class:"baseline-fix"},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),a("​")])])]),s("span",{class:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"})]),s("span",{class:"mrel"},"≈"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"."),s("span",{class:"mord mathrm"},"2"),s("span",{class:"mord mathrm"},"0"),s("span",{class:"mord mathrm"},"8")])])])])]),s("ul",null,[s("li",null,[s("strong",null,"关键洞见"),a("：即使检测阳性，患癌概率仅20.8%，凸显先验概率的重要性。")])])])],-1),t(`<hr><h3 id="二、贝叶斯学习的两大核心方法" tabindex="-1"><strong>二、贝叶斯学习的两大核心方法</strong> <a class="header-anchor" href="#二、贝叶斯学习的两大核心方法" aria-label="Permalink to &quot;**二、贝叶斯学习的两大核心方法**&quot;">​</a></h3><h4 id="_1-极大后验假设-map" tabindex="-1"><strong>1. 极大后验假设（MAP）</strong> <a class="header-anchor" href="#_1-极大后验假设-map" aria-label="Permalink to &quot;**1. 极大后验假设（MAP）**&quot;">​</a></h4><ul><li><strong>目标</strong>：选择使后验概率最大的假设</li></ul><pre><code>$$ h_{MAP} = \\arg\\max_h P(h|D) = \\arg\\max_h P(D|h)P(h)$$
</code></pre><ul><li><strong>特点</strong>： <ul><li>融合先验知识（如医生经验）与观测数据</li><li>在癌症例子中，$$( h_{MAP} = \\neg\\text{cancer} ) ( P(+) )$$ 主要由健康人群的假阳性主导</li></ul></li></ul><h4 id="_2-极大似然假设-ml" tabindex="-1"><strong>2. 极大似然假设（ML）</strong> <a class="header-anchor" href="#_2-极大似然假设-ml" aria-label="Permalink to &quot;**2. 极大似然假设（ML）**&quot;">​</a></h4><ul><li><strong>目标</strong>：忽略先验，仅最大化似然度<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>M</mi><mi>L</mi></mrow></msub><mo>=</mo><mi>arg</mi><msub><mi>max</mi><mi>h</mi></msub><mi>P</mi><mo>(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>h</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">h_{ML} = \\arg\\max_h P(D|h) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.502108em;vertical-align:-0.752108em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mord mathit">L</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mop op-limits"><span class="vlist"><span style="top:0.652108em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">h</span></span></span><span style="top:8.326672684688674e-17em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="mop">max</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mord mathrm">∣</span><span class="mord mathit">h</span><span class="mclose">)</span></span></span></span></span></p></li><li><strong>适用场景</strong>： <ul><li>数据量充足时，先验影响减弱</li><li>与最小二乘（LSE）的关系：在高斯噪声假设下，ML等价于LSE</li></ul></li></ul><h4 id="对比总结" tabindex="-1"><strong>对比总结</strong> <a class="header-anchor" href="#对比总结" aria-label="Permalink to &quot;**对比总结**&quot;">​</a></h4><table tabindex="0"><thead><tr><th>方法</th><th>是否利用先验</th><th>数据量少时的表现</th><th>计算复杂度</th></tr></thead><tbody><tr><td>MAP</td><td>是</td><td>更稳健</td><td>较高（需估计先验）</td></tr><tr><td>ML</td><td>否</td><td>可能过拟合</td><td>较低</td></tr></tbody></table><hr><h3 id="三、朴素贝叶斯-naive-bayes-nb" tabindex="-1"><strong>三、朴素贝叶斯（Naïve Bayes, NB）</strong> <a class="header-anchor" href="#三、朴素贝叶斯-naive-bayes-nb" aria-label="Permalink to &quot;**三、朴素贝叶斯（Naïve Bayes, NB）**&quot;">​</a></h3><ol><li><p><strong>核心假设</strong></p><ul><li>特征条件独立：给定类别，各特征独立影响结果<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo>)</mo><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>P</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(x_1, x_2, ..., x_n | y) = \\prod_{i=1}^n P(x_i | y) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.6513970000000002em;"></span><span class="strut bottom" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∏</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p></li><li><strong>优势</strong>： <ul><li>计算高效，尤其适合高维数据（如文本分类）</li><li>需估计的参数数量从指数级降至线性级</li></ul></li></ul></li><li><p><strong>NB vs MAP</strong></p><ul><li><strong>NB</strong>：强制独立性简化计算，牺牲精度换取效率</li><li><strong>MAP</strong>：无独立性假设，但需完整联合概率分布（实际中常不可行）</li></ul></li></ol><hr><h3 id="四、最小描述长度-mdl-原理" tabindex="-1"><strong>四、最小描述长度（MDL）原理</strong> <a class="header-anchor" href="#四、最小描述长度-mdl-原理" aria-label="Permalink to &quot;**四、最小描述长度（MDL）原理**&quot;">​</a></h3><ol><li><p><strong>奥卡姆剃刀形式化</strong></p><ul><li>最优假设 ( h ) 最小化：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>C</mi></msub><mo>(</mo><mi>h</mi><mo>)</mo><mo>+</mo><msub><mi>L</mi><mi>C</mi></msub><mo>(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>h</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">L_C(h) + L_C(D|h) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mord mathrm">∣</span><span class="mord mathit">h</span><span class="mclose">)</span></span></span></span></span></p> 其中 ( L_C ) 为编码长度，平衡模型复杂度与拟合误差。</li></ul></li><li><p><strong>信息论解释</strong></p><ul><li><strong>消息编码</strong>：高频事件用短码（如霍夫曼编码），最优码长 ( -\\log_2 p_i )</li><li><strong>假设选择</strong>： <ul><li>简单假设（短编码）但分类错误多 → 需额外传输纠错信息</li><li>复杂假设（长编码）但分类精准 → 纠错开销低</li></ul></li></ul></li><li><p><strong>MDL与MAP的关联</strong></p><ul><li>当概率分布与编码方案一致时，MDL等价于最大化后验概率（对数形式）：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>arg</mi><msub><mi>min</mi><mi>h</mi></msub><mo>[</mo><mo>−</mo><mi>log</mi><mi>P</mi><mo>(</mo><mi>h</mi><mo>)</mo><mo>−</mo><mi>log</mi><mi>P</mi><mo>(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>h</mi><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">\\arg\\min_h [ -\\log P(h) - \\log P(D|h) ] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.5021079999999998em;vertical-align:-0.7521079999999999em;"></span><span class="base displaystyle textstyle uncramped"><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mop op-limits"><span class="vlist"><span style="top:0.6521079999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">h</span></span></span><span style="top:-2.7755575615628914e-17em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="mop">min</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mbin">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="mord mathrm">∣</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p></li></ul></li></ol><hr><h3 id="五、关键问题深度探讨" tabindex="-1"><strong>五、关键问题深度探讨</strong> <a class="header-anchor" href="#五、关键问题深度探讨" aria-label="Permalink to &quot;**五、关键问题深度探讨**&quot;">​</a></h3><ol><li><p><strong>为什么医学检测需要贝叶斯？</strong></p><ul><li>罕见病（低先验）下，即使高精度检测也可能因假阳性导致低后验概率。</li><li><strong>举例</strong>：若癌症患病率降至0.1%，阳性检测后患癌概率仅约3.2%。</li></ul></li><li><p><strong>独立性假设的代价</strong></p><ul><li>若特征实际相关（如“咳嗽”和“发烧”），NB会低估联合概率。</li><li><strong>改进方法</strong>：半朴素贝叶斯（部分依赖）、贝叶斯网络（显式建模依赖）。</li></ul></li><li><p><strong>贝叶斯与频率学派的哲学差异</strong></p><ul><li>贝叶斯视角：概率即主观信念，可迭代更新</li><li>频率派视角：概率是长期频率，假设固定但未知</li></ul></li></ol><hr><h3 id="六、实践建议" tabindex="-1"><strong>六、实践建议</strong> <a class="header-anchor" href="#六、实践建议" aria-label="Permalink to &quot;**六、实践建议**&quot;">​</a></h3><ol><li><p><strong>选择方法的准则</strong></p><ul><li>数据少、先验可靠 → MAP</li><li>数据多、特征独立 → NB</li><li>需理论保证的泛化性 → MDL</li></ul></li><li><p><strong>代码实现提示</strong></p><ul><li>对数空间计算：避免概率连乘的下溢（使用 <code>np.log</code> 相加替代乘法）</li><li>拉普拉斯平滑：处理零概率问题（如未登录词）</li></ul></li></ol>`,22)]))}const y=e(p,[["render",i]]);export{g as __pageData,y as default};
