import{_ as r,c as m,o as t,a3 as l,j as s,a}from"./chunks/framework.C8Xs1bna.js";const g=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"AI/1.机器学习/06.基于实例的学习方法/在无限样本下，1-NN的错误率不超过贝叶斯最优错误率的2倍的解析.md","filePath":"AI/1.机器学习/06.基于实例的学习方法/在无限样本下，1-NN的错误率不超过贝叶斯最优错误率的2倍的解析.md","lastUpdated":1754900608000}'),e={name:"AI/1.机器学习/06.基于实例的学习方法/在无限样本下，1-NN的错误率不超过贝叶斯最优错误率的2倍的解析.md"};function o(i,n,c,p,d,u){return t(),m("div",null,n[0]||(n[0]=[l('<p>这个理论结果（Cover &amp; Hart, 1967）是<strong>最近邻（1-NN）算法</strong>最重要的理论保证之一。它的含义是：<strong>在训练样本数量趋于无穷大时，1-NN分类器的错误率（Err(1-NN)）不会超过贝叶斯最优分类器错误率（Err(Bayes)）的2倍。</strong> 理解这个不等式需要拆解几个关键概念：</p><hr><h3 id="_1-贝叶斯最优错误率-err-bayes" tabindex="-1"><strong>1. 贝叶斯最优错误率 (Err(Bayes))</strong> <a class="header-anchor" href="#_1-贝叶斯最优错误率-err-bayes" aria-label="Permalink to &quot;**1. 贝叶斯最优错误率 (Err(Bayes))**&quot;">​</a></h3><ul><li><strong>定义</strong>：这是理论上可达到的<strong>最低错误率</strong>，是任何分类器都无法超越的极限。</li><li><strong>原理</strong>：贝叶斯分类器根据样本 <code>x</code> 属于每个类别 <code>c</code> 的<strong>后验概率</strong> <code>P(c|x)</code> 做决策，总是选择后验概率最大的类别：</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>c</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><msub><mi>max</mi><mrow><mi>c</mi></mrow></msub><mi>P</mi><mo>(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex"> c^* = \\arg\\max_{c} P(c|x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.45em;vertical-align:-0.7em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">c</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mop op-limits"><span class="vlist"><span style="top:0.6em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span></span></span></span><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="mop">max</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li><strong>错误来源</strong>：即使知道真实的 <code>P(c|x)</code>，错误仍然可能发生。当 <code>x</code> 的真实类别不是 <code>P(c|x)</code> 最大的那个类别时，贝叶斯分类器也会犯错。这种固有的不确定性导致的错误率就是 <code>Err(Bayes)</code>。</li></ul><hr><h3 id="_2-1-nn-的错误率-err-1-nn" tabindex="-1"><strong>2. 1-NN 的错误率 (Err(1-NN))</strong> <a class="header-anchor" href="#_2-1-nn-的错误率-err-1-nn" aria-label="Permalink to &quot;**2. 1-NN 的错误率 (Err(1-NN))**&quot;">​</a></h3><ul><li><strong>定义</strong>：使用最近邻规则（只找最近的一个邻居）进行分类的错误率。</li><li><strong>原理</strong>：对查询点 <code>x_query</code>，找到训练集中离它最近的样本 <code>x_nn</code>，然后将 <code>x_nn</code> 的标签赋予 <code>x_query</code>。</li></ul><hr><h3 id="_3-不等式-err-bayes-≤-err-1-nn-≤-2-×-err-bayes-的直观解释" tabindex="-1"><strong>3. 不等式 <code>Err(Bayes) ≤ Err(1-NN) ≤ 2 × Err(Bayes)</code> 的直观解释</strong> <a class="header-anchor" href="#_3-不等式-err-bayes-≤-err-1-nn-≤-2-×-err-bayes-的直观解释" aria-label="Permalink to &quot;**3. 不等式 `Err(Bayes) ≤ Err(1-NN) ≤ 2 × Err(Bayes)` 的直观解释**&quot;">​</a></h3>',11),s("ol",null,[s("li",null,[s("p",null,[s("strong",null,[a("下界 "),s("code",null,"Err(Bayes) ≤ Err(1-NN)")]),a(":")]),s("ul",null,[s("li",null,[a("这是显然成立的。因为贝叶斯错误率 "),s("code",null,"Err(Bayes)"),a(" 是理论最优的、任何分类器能达到的最低错误率。")]),s("li",null,[a("1-NN 作为一个具体的分类器，其错误率 "),s("code",null,"Err(1-NN)"),a(" 不可能低于这个理论极限。")])])]),s("li",null,[s("p",null,[s("strong",null,[a("上界 "),s("code",null,"Err(1-NN) ≤ 2 × Err(Bayes)")]),a(":")]),s("ul",null,[s("li",null,[s("strong",null,"核心思想"),a("：当训练数据无限多时，点 "),s("code",null,"x_query"),a(" 的最近邻点 "),s("code",null,"x_nn"),a(" 会无限接近它 ("),s("code",null,"x_nn"),a(" ≈ "),s("code",null,"x_query"),a(")。此时，1-NN 出错的原因几乎完全等同于贝叶斯分类器在 "),s("code",null,"x_query"),a(" 和 "),s("code",null,"x_nn"),a(" 这两个点上出错的概率之和。")]),s("li",null,[s("strong",null,"推导思路（简化）"),a("： "),s("ol",null,[s("li",null,[s("strong",null,"1-NN 出错的场景"),a("：当 "),s("code",null,"x_query"),a(" 的真实类别 "),s("code",null,"c_true"),a(" 与其最近邻 "),s("code",null,"x_nn"),a(" 的类别 "),s("code",null,"c_nn"),a(" 不同时 ("),s("code",null,"c_true ≠ c_nn"),a(")。")]),s("li",null,[s("strong",null,"关键观察"),a("：如果 "),s("code",null,"c_true ≠ c_nn"),a("，那么对于点 "),s("code",null,"x_query"),a(" 和点 "),s("code",null,"x_nn"),a("，至少有一个点会被贝叶斯分类器分类错误！ "),s("ul",null,[s("li",null,[a("情况 A："),s("code",null,"x_query"),a(" 被贝叶斯分类器分错（概率 ≈ "),s("code",null,"Err(Bayes)"),a("）。")]),s("li",null,[a("情况 B："),s("code",null,"x_nn"),a(" 被贝叶斯分类器分错（概率 ≈ "),s("code",null,"Err(Bayes)"),a("）。")]),s("li",null,[s("strong",null,"不可能发生的情况"),a("："),s("code",null,"c_true ≠ c_nn"),a("，但贝叶斯分类器同时把 "),s("code",null,"x_query"),a(" 和 "),s("code",null,"x_nn"),a(" 都分对。因为如果贝叶斯分类器在 "),s("code",null,"x_query"),a(" 和 "),s("code",null,"x_nn"),a(" 上都分对了，那就意味着 "),s("code",null,"c_true = c_query"),a(" 且 "),s("code",null,"c_nn = c_true"),a("，这与 "),s("code",null,"c_true ≠ c_nn"),a(" 矛盾。")])])]),s("li",null,[s("strong",null,"概率关系"),a("：因此，事件 "),s("code",null,"c_true ≠ c_nn"),a("（导致 1-NN 出错）发生的概率，小于等于事件“"),s("code",null,"x_query"),a(" 被贝叶斯分错”或“"),s("code",null,"x_nn"),a(" 被贝叶斯分错”发生的概率（即两者的并集）。")]),s("li",null,[s("strong",null,"概率上界"),a("：两个独立事件（在极限下，点之间独立）并集的概率小于等于它们各自概率之和："),s("p",null,[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",null,[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",null,"("),s("mtext",null,[s("mn",null,"1"),s("mi",{mathvariant:"normal"},"-"),s("mi",{mathvariant:"normal"},"N"),s("mi",{mathvariant:"normal"},"N"),s("mtext",null," "),s("mi",{mathvariant:"normal"},"e"),s("mi",{mathvariant:"normal"},"r"),s("mi",{mathvariant:"normal"},"r"),s("mi",{mathvariant:"normal"},"o"),s("mi",{mathvariant:"normal"},"r")]),s("mo",null,")"),s("mo",null,"≤"),s("mi",null,"P"),s("mo",null,"("),s("mtext",null,[s("mi",{mathvariant:"normal"},"B"),s("mi",{mathvariant:"normal"},"a"),s("mi",{mathvariant:"normal"},"y"),s("mi",{mathvariant:"normal"},"e"),s("mi",{mathvariant:"normal"},"s"),s("mtext",null," "),s("mi",{mathvariant:"normal"},"e"),s("mi",{mathvariant:"normal"},"r"),s("mi",{mathvariant:"normal"},"r"),s("mi",{mathvariant:"normal"},"o"),s("mi",{mathvariant:"normal"},"r"),s("mtext",null," "),s("mi",{mathvariant:"normal"},"a"),s("mi",{mathvariant:"normal"},"t"),s("mtext",null," ")]),s("msub",null,[s("mi",null,"x"),s("mrow",null,[s("mi",null,"q"),s("mi",null,"u"),s("mi",null,"e"),s("mi",null,"r"),s("mi",null,"y")])]),s("mo",null,")"),s("mo",null,"+"),s("mi",null,"P"),s("mo",null,"("),s("mtext",null,[s("mi",{mathvariant:"normal"},"B"),s("mi",{mathvariant:"normal"},"a"),s("mi",{mathvariant:"normal"},"y"),s("mi",{mathvariant:"normal"},"e"),s("mi",{mathvariant:"normal"},"s"),s("mtext",null," "),s("mi",{mathvariant:"normal"},"e"),s("mi",{mathvariant:"normal"},"r"),s("mi",{mathvariant:"normal"},"r"),s("mi",{mathvariant:"normal"},"o"),s("mi",{mathvariant:"normal"},"r"),s("mtext",null," "),s("mi",{mathvariant:"normal"},"a"),s("mi",{mathvariant:"normal"},"t"),s("mtext",null," ")]),s("msub",null,[s("mi",null,"x"),s("mrow",null,[s("mi",null,"n"),s("mi",null,"n")])]),s("mo",null,")"),s("mo",null,"≈"),s("mi",null,"E"),s("mi",null,"r"),s("mi",null,"r"),s("mo",null,"("),s("mi",null,"B"),s("mi",null,"a"),s("mi",null,"y"),s("mi",null,"e"),s("mi",null,"s"),s("mo",null,")"),s("mo",null,"+"),s("mi",null,"E"),s("mi",null,"r"),s("mi",null,"r"),s("mo",null,"("),s("mi",null,"B"),s("mi",null,"a"),s("mi",null,"y"),s("mi",null,"e"),s("mi",null,"s"),s("mo",null,")"),s("mo",null,"="),s("mn",null,"2"),s("mo",null,"×"),s("mi",null,"E"),s("mi",null,"r"),s("mi",null,"r"),s("mo",null,"("),s("mi",null,"B"),s("mi",null,"a"),s("mi",null,"y"),s("mi",null,"e"),s("mi",null,"s"),s("mo",null,")")]),s("annotation",{encoding:"application/x-tex"},"P(\\text{1-NN error}) \\leq P(\\text{Bayes error at } x_{query}) + P(\\text{Bayes error at } x_{nn}) \\approx Err(Bayes) + Err(Bayes) = 2 \\times Err(Bayes) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"strut",style:{height:"0.75em"}}),s("span",{class:"strut bottom",style:{height:"1.036108em","vertical-align":"-0.286108em"}}),s("span",{class:"base displaystyle textstyle uncramped"},[s("span",{class:"mord mathit",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"text mord displaystyle textstyle uncramped"},[s("span",{class:"mord mathrm"},"1"),s("span",{class:"mord mathrm"},"-"),s("span",{class:"mord mathrm"},"N"),s("span",{class:"mord mathrm"},"N"),s("span",{class:"mord mspace"}," "),s("span",{class:"mord mathrm"},"e"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mathrm"},"o"),s("span",{class:"mord mathrm"},"r")]),s("span",{class:"mclose"},")"),s("span",{class:"mrel"},"≤"),s("span",{class:"mord mathit",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"text mord displaystyle textstyle uncramped"},[s("span",{class:"mord mathrm"},"B"),s("span",{class:"mord mathrm"},"a"),s("span",{class:"mord mathrm",style:{"margin-right":"0.01389em"}},"y"),s("span",{class:"mord mathrm"},"e"),s("span",{class:"mord mathrm"},"s"),s("span",{class:"mord mspace"}," "),s("span",{class:"mord mathrm"},"e"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mathrm"},"o"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mspace"}," "),s("span",{class:"mord mathrm"},"a"),s("span",{class:"mord mathrm"},"t"),s("span",{class:"mord mspace"}," ")]),s("span",{class:"mord"},[s("span",{class:"mord mathit"},"x"),s("span",{class:"vlist"},[s("span",{style:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),s("span",{class:"reset-textstyle scriptstyle cramped"},[s("span",{class:"mord scriptstyle cramped"},[s("span",{class:"mord mathit",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord mathit"},"u"),s("span",{class:"mord mathit"},"e"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathit",style:{"margin-right":"0.03588em"}},"y")])])]),s("span",{class:"baseline-fix"},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),a("​")])])]),s("span",{class:"mclose"},")"),s("span",{class:"mbin"},"+"),s("span",{class:"mord mathit",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"text mord displaystyle textstyle uncramped"},[s("span",{class:"mord mathrm"},"B"),s("span",{class:"mord mathrm"},"a"),s("span",{class:"mord mathrm",style:{"margin-right":"0.01389em"}},"y"),s("span",{class:"mord mathrm"},"e"),s("span",{class:"mord mathrm"},"s"),s("span",{class:"mord mspace"}," "),s("span",{class:"mord mathrm"},"e"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mathrm"},"o"),s("span",{class:"mord mathrm"},"r"),s("span",{class:"mord mspace"}," "),s("span",{class:"mord mathrm"},"a"),s("span",{class:"mord mathrm"},"t"),s("span",{class:"mord mspace"}," ")]),s("span",{class:"mord"},[s("span",{class:"mord mathit"},"x"),s("span",{class:"vlist"},[s("span",{style:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),s("span",{class:"reset-textstyle scriptstyle cramped"},[s("span",{class:"mord scriptstyle cramped"},[s("span",{class:"mord mathit"},"n"),s("span",{class:"mord mathit"},"n")])])]),s("span",{class:"baseline-fix"},[s("span",{class:"fontsize-ensurer reset-size5 size5"},[s("span",{style:{"font-size":"0em"}},"​")]),a("​")])])]),s("span",{class:"mclose"},")"),s("span",{class:"mrel"},"≈"),s("span",{class:"mord mathit",style:{"margin-right":"0.05764em"}},"E"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathit",style:{"margin-right":"0.05017em"}},"B"),s("span",{class:"mord mathit"},"a"),s("span",{class:"mord mathit",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"mord mathit"},"e"),s("span",{class:"mord mathit"},"s"),s("span",{class:"mclose"},")"),s("span",{class:"mbin"},"+"),s("span",{class:"mord mathit",style:{"margin-right":"0.05764em"}},"E"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathit",style:{"margin-right":"0.05017em"}},"B"),s("span",{class:"mord mathit"},"a"),s("span",{class:"mord mathit",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"mord mathit"},"e"),s("span",{class:"mord mathit"},"s"),s("span",{class:"mclose"},")"),s("span",{class:"mrel"},"="),s("span",{class:"mord mathrm"},"2"),s("span",{class:"mbin"},"×"),s("span",{class:"mord mathit",style:{"margin-right":"0.05764em"}},"E"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathit",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathit",style:{"margin-right":"0.05017em"}},"B"),s("span",{class:"mord mathit"},"a"),s("span",{class:"mord mathit",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"mord mathit"},"e"),s("span",{class:"mord mathit"},"s"),s("span",{class:"mclose"},")")])])])])])])])]),s("li",null,[a("这就解释了上界 "),s("code",null,"Err(1-NN) ≤ 2 × Err(Bayes)"),a(" 的来源。")])])])],-1),l('<hr><h3 id="_4-重要前提条件" tabindex="-1"><strong>4. 重要前提条件</strong> <a class="header-anchor" href="#_4-重要前提条件" aria-label="Permalink to &quot;**4. 重要前提条件**&quot;">​</a></h3><ul><li><strong>无限训练样本 (<code>n → ∞</code>)</strong>：这是理论成立的关键。只有在数据点无限密集地覆盖整个特征空间时，“最近邻点无限接近查询点”的假设才成立。</li><li><strong>独立同分布 (i.i.d.)</strong>：训练样本和测试样本必须独立且来自相同的概率分布。</li><li><strong>度量空间</strong>：需要定义合适的距离度量（如欧氏距离）。</li></ul><hr><h3 id="_5-意义与启示" tabindex="-1"><strong>5. 意义与启示</strong> <a class="header-anchor" href="#_5-意义与启示" aria-label="Permalink to &quot;**5. 意义与启示**&quot;">​</a></h3><ul><li><strong>理论保证</strong>：它证明了即使是最简单的 1-NN 规则，在数据足够多时，性能也不会太差（错误率最多是最优分类器的2倍）。</li><li><strong>渐进性质</strong>：这是一个<strong>渐近结果</strong>，描述了当样本量趋向无穷大时的极限行为。实际应用中，有限样本下的性能可能远差于这个界限（尤其在高维空间）。</li><li><strong>KNN 的改进空间</strong>：这个界限是针对 <code>k=1</code> 的。使用更大的 <code>k</code>（如KNN），通过投票机制可以平滑噪声，通常能在有限样本下获得比1-NN更好的性能，甚至更接近贝叶斯错误率（虽然其理论界限可能没有1-NN这么简洁）。</li><li><strong>贝叶斯错误率是基准</strong>：它强调了 <code>Err(Bayes)</code> 是衡量分类问题难度和分类器性能的根本基准。</li></ul><hr><p><strong>总结</strong>：这个不等式 <code>Err(Bayes) ≤ Err(1-NN) ≤ 2 × Err(Bayes)</code> 表明，在拥有海量训练数据的前提下，1-NN 分类器的性能虽然无法达到理论最优（贝叶斯分类器），但其错误率最坏情况下也不会超过最优错误率的两倍。这为基于实例的学习方法（特别是最近邻规则）提供了坚实的理论基础。</p>',8)]))}const y=r(e,[["render",o]]);export{g as __pageData,y as default};
